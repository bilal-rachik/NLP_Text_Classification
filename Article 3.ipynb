{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP (Natural Language Processing)-machine learning Classification  3( N-gram vectors)\n",
    "\n",
    "Dans l'article précedent, nous avons décomposé le workflow de classification de texte en plusieurs étapes. pour répondre à deux questions clés:\n",
    "\n",
    "\n",
    "* Quel algorithme ou modèle d'apprentissage devrions-nous utiliser?\n",
    "\n",
    "* Comment préparer les données pour apprendre efficacement la relation entre le texte et l'étiquette?\n",
    "\n",
    "\n",
    "Pour chaque étape, nous avons proposé une approche personnalisée basée sur les caractéristiques spécifique de votre jeu de données. En particulier, en utilisant le rapport du nombre d'échantillons au nombre de mots par échantillon, nous avons suggéré deux approche .\n",
    "\n",
    "La réponse à la deuxième question dépend de la réponse à la première question, la façon dont nous pré-traitons les données à introduire dans un modèle dépendra du modèle que nous choisirons. Les modèles peuvent être classés en deux grandes catégories:\n",
    "\n",
    "* ceux qui utilisent le texte comme une séquence des mots (modèles de séquence) \n",
    "\n",
    "* ceux qui ne voient que le texte comme des «sac de mots>>  (modèles à n-grammes). \n",
    "\n",
    "Lorsque la valeur de ce rapport est petite (<1500), les modèles  qui prennent les n-grammes en entrée  fonctionnent mieux ou au moins aussi bien que les modèles de séquence.\n",
    "Lorsque la valeur de ce rapport est grande (> = 1500), utilisez un modèle de séquence . \n",
    "\n",
    "Dans cet article , nous travaillerons à la construction, et à l'évaluation des modèles à n-grammes,\n",
    "Les modèles  qui prennent les n-grammes en entrée sont simples à définir et à comprendre, et ils prennent moins de temps de calcul que les modèles de séquence.\n",
    "\n",
    "Certains des algorithmes de classification à n-grammes les plus largement utilisés sont la régression logistique,les perceptrons multicouches(MLP),  Naive Bayes, le plus proche voisin K et les méthodes d'arbre.... ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 4: Créez, formez et évaluez votre modèle\n",
    "\n",
    "À l' étape 3 , nous avons choisi d'utiliser un modèle à n-grammes ou un modèle de séquence.\n",
    "Maintenant, il est temps d'écrire notre algorithme de classification et de le former.\n",
    "\n",
    "#### Construire un modèle n-gram\n",
    "\n",
    "Nous nous référons aux modèles qui traitent les tokens indépendamment (sans tenir compte de l'ordre des mots) en tant que modèles à n-grammes. Les perceptrons multicouches  (y compris la régression logistique ), Naive Bayes, le plus proche voisin K et les méthodes d'arbre.... , entrent tous dans cette catégorie\n",
    "\n",
    "Découvrons chacun d'eux ensembles\n",
    "\n",
    "On recupere tout d'abord les données, et de créer deux DataFrame Pandas, un pour l'apprentissage, l'autre pour la validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as sms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(input_path, nb_line, tauxValid):\n",
    "    data_all = pd.read_csv(input_path,sep=\",\", nrows=nb_line)\n",
    "    data_all = data_all.fillna(\"\")\n",
    "    data_train, data_valid = sms.train_test_split(data_all, test_size=tauxValid, random_state=47, shuffle=True)\n",
    "    return data_train, data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set : 40000 elements, Validation set : 10000 elements\n"
     ]
    }
   ],
   "source": [
    "input_path = \"data/cdiscount_train.csv.zip\"\n",
    "nb_line=50000 # part totale extraite du fichier initial ici déjà réduit\n",
    "tauxValid = 0.2\n",
    "data_train, data_valid = split_dataset(input_path, nb_line, tauxValid)\n",
    "data_train.reset_index(inplace = True)\n",
    "data_valid.reset_index(inplace = True)\n",
    "\n",
    "N_train = data_train.shape[0]\n",
    "N_valid = data_valid.shape[0]\n",
    "print(\"Train set : %d elements, Validation set : %d elements\" %(N_train, N_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectoriser en utilisant l'encodage tf-idf,\n",
    "Sélectionnez uniquement les 20 000 premières fonctionnalités du vecteur de jetons en supprimant les jetons qui apparaissent moins de 2 fois et en utilisant f_classif pour calculer l'importance des fonctionnalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "nlp = French()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [word.lemma_.lower() for word in mytokens if\n",
    "                not word.is_punct and not word.like_num and word.text != 'n']\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [word for word in mytokens if word not in stop_words]\n",
    "\n",
    "    # Remove accentuated char for any unicode symbol\n",
    "    mytokens = [strip_accents_ascii(word) for word in mytokens]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_val = ngram_vectorize(data_train[\"Description\"], data_train[\"Categorie1\"], data_valid[\"Description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bagging\n",
    "\n",
    "Le principe des méthodes de Bagging, et donc en particulier des forêts aléatoires c’est de faire la moyenne des prévisions de plusieurs modèles indépendants pour réduire la variance et donc l’erreur de prévision. Pour construire ces différents modèles, on sélectionne plusieurs échantillons bootstrap, c’est à dire des tirages avec remises.\n",
    "\n",
    "![](data/bagging.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        ADULTE - EROTIQUE        0.57      0.23      0.33        98\n",
      "                          ANIMALERIE - NEW       0.55      0.28      0.37        64\n",
      "            ARME DE COMBAT - ARME DE SPORT       1.00      0.11      0.20         9\n",
      "     ART DE LA TABLE - ARTICLES CULINAIRES       0.45      0.33      0.38       111\n",
      "                      ARTICLES POUR FUMEUR       0.84      0.40      0.54        40\n",
      "                         AUTO - MOTO (NEW)       0.77      0.77      0.77       442\n",
      "                                 BAGAGERIE       0.81      0.81      0.81       215\n",
      "                   BATEAU MOTEUR - VOILIER       0.00      0.00      0.00         6\n",
      "              BIJOUX -  LUNETTES - MONTRES       0.87      0.94      0.90       591\n",
      "     BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.31      0.54      0.39       274\n",
      "                  CHAUSSURES - ACCESSOIRES       0.81      0.65      0.72       161\n",
      "                           CONDITIONNEMENT       0.00      0.00      0.00         2\n",
      "                            CULTURE / JEUX       0.80      0.73      0.76       160\n",
      "                  DECO - LINGE - LUMINAIRE       0.63      0.75      0.69       561\n",
      "                           DROGUERIE (NEW)       0.00      0.00      0.00        13\n",
      "                            ELECTROMENAGER       0.88      0.76      0.82       217\n",
      "                              ELECTRONIQUE       0.90      0.64      0.75        28\n",
      "                                  EPICERIE       0.22      0.08      0.12        25\n",
      "                 HYGIENE - BEAUTE - PARFUM       0.54      0.40      0.46       124\n",
      "                              INFORMATIQUE       0.93      0.92      0.92      1467\n",
      "                    INSTRUMENTS DE MUSIQUE       0.30      0.10      0.15        30\n",
      "                          JARDIN - PISCINE       0.76      0.40      0.52       153\n",
      "                               JOUET (NEW)       0.45      0.38      0.41       211\n",
      "                                 LIBRAIRIE       0.76      0.98      0.86      1194\n",
      "                                   LITERIE       0.79      0.71      0.75        38\n",
      " LOISIRS CREATIFS - BEAUX ARTS - PAPETERIE       0.74      0.28      0.41       100\n",
      "                               MANUTENTION       0.00      0.00      0.00         2\n",
      "                        MATERIEL DE BUREAU       0.00      0.00      0.00         6\n",
      "                          MATERIEL MEDICAL       0.00      0.00      0.00         4\n",
      "                                  MERCERIE       0.64      0.32      0.42        22\n",
      "                                   MEUBLE        0.74      0.65      0.69       136\n",
      "                             PARAPHARMACIE       0.20      0.03      0.06        30\n",
      "                           PHOTO - OPTIQUE       0.84      0.56      0.67       176\n",
      "POINT DE VENTE - COMMERCE - ADMINISTRATION       0.47      0.62      0.53        13\n",
      "                              PUERICULTURE       0.53      0.22      0.31        45\n",
      "                                 SONO - DJ       0.00      0.00      0.00        15\n",
      "                               SPORT (NEW)       0.45      0.36      0.40       267\n",
      "                       TATOUAGE - PIERCING       1.00      0.71      0.83        14\n",
      "                          TELEPHONIE - GPS       0.95      0.97      0.96      2183\n",
      "                     TENUE PROFESSIONNELLE       0.00      0.00      0.00         1\n",
      "                          TV - VIDEO - SON       0.70      0.30      0.42       109\n",
      "                     VETEMENTS - LINGERIE        0.71      0.79      0.74       618\n",
      "                   VIN - ALCOOL - LIQUIDES       0.67      0.08      0.14        25\n",
      "\n",
      "                                  accuracy                           0.79     10000\n",
      "                                 macro avg       0.55      0.41      0.45     10000\n",
      "                              weighted avg       0.78      0.79      0.77     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "y_train = data_train[\"Categorie1\"]\n",
    "\n",
    "y_test = data_valid[\"Categorie1\"]\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=50, n_jobs=-1)\n",
    "bag_clf.fit(x_train, y_train)\n",
    "\n",
    "predicted = bag_clf.predict(x_val)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Random Forest\n",
    "\n",
    "Cet algorithme appartient à la famille des agrégations de modèles, c’est en fait un cas particulier de bagging (bootstrap aggregating) appliqué aux arbres de décision de type CART. \n",
    "En plus du principe de bagging, les forêts aléatoires ajoutent de l’aléa au niveau des variables. Pour chaque arbre on sélectionne un échantillon bootstrat d’individus et à chaque étape, la construction d’un noeud de l’arbre se fait sur un sous-ensemble de variables tirées aléatoirement.\n",
    "\n",
    "On se retrouve donc avec plusieurs arbres et donc des prédictions différentes pour chaque individu. Comment obtenir l’estimation finale?\n",
    "\n",
    "* Dans le cas d’une classification : on choisit la catégorie la plus fréquente\n",
    "* Dans le cas d’une régression : on fait la moyenne des valeurs prédites\n",
    "\n",
    "![](data/forest.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        ADULTE - EROTIQUE        0.69      0.34      0.45        98\n",
      "                          ANIMALERIE - NEW       0.74      0.39      0.51        64\n",
      "            ARME DE COMBAT - ARME DE SPORT       1.00      0.44      0.62         9\n",
      "     ART DE LA TABLE - ARTICLES CULINAIRES       0.80      0.36      0.50       111\n",
      "                      ARTICLES POUR FUMEUR       0.83      0.62      0.71        40\n",
      "                         AUTO - MOTO (NEW)       0.83      0.79      0.81       442\n",
      "                                 BAGAGERIE       0.82      0.82      0.82       215\n",
      "                   BATEAU MOTEUR - VOILIER       0.00      0.00      0.00         6\n",
      "              BIJOUX -  LUNETTES - MONTRES       0.89      0.94      0.92       591\n",
      "     BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.43      0.53      0.48       274\n",
      "                  CHAUSSURES - ACCESSOIRES       0.88      0.75      0.81       161\n",
      "                           CONDITIONNEMENT       0.00      0.00      0.00         2\n",
      "                            CULTURE / JEUX       0.76      0.76      0.76       160\n",
      "                  DECO - LINGE - LUMINAIRE       0.66      0.80      0.72       561\n",
      "                           DROGUERIE (NEW)       0.67      0.31      0.42        13\n",
      "                            ELECTROMENAGER       0.91      0.78      0.84       217\n",
      "                              ELECTRONIQUE       0.87      0.71      0.78        28\n",
      "                                  EPICERIE       0.40      0.16      0.23        25\n",
      "                 HYGIENE - BEAUTE - PARFUM       0.69      0.58      0.63       124\n",
      "                              INFORMATIQUE       0.90      0.91      0.90      1467\n",
      "                    INSTRUMENTS DE MUSIQUE       0.80      0.40      0.53        30\n",
      "                          JARDIN - PISCINE       0.81      0.39      0.52       153\n",
      "                               JOUET (NEW)       0.58      0.39      0.47       211\n",
      "                                 LIBRAIRIE       0.71      0.98      0.82      1194\n",
      "                                   LITERIE       0.85      0.87      0.86        38\n",
      " LOISIRS CREATIFS - BEAUX ARTS - PAPETERIE       0.73      0.35      0.47       100\n",
      "                               MANUTENTION       0.00      0.00      0.00         2\n",
      "                        MATERIEL DE BUREAU       0.00      0.00      0.00         6\n",
      "                          MATERIEL MEDICAL       0.00      0.00      0.00         4\n",
      "                                  MERCERIE       0.93      0.64      0.76        22\n",
      "                                   MEUBLE        0.79      0.69      0.74       136\n",
      "                             PARAPHARMACIE       0.67      0.13      0.22        30\n",
      "                           PHOTO - OPTIQUE       0.91      0.57      0.70       176\n",
      "POINT DE VENTE - COMMERCE - ADMINISTRATION       0.47      0.54      0.50        13\n",
      "                              PUERICULTURE       0.63      0.38      0.47        45\n",
      "                                 SONO - DJ       0.00      0.00      0.00        15\n",
      "                               SPORT (NEW)       0.60      0.33      0.43       267\n",
      "                       TATOUAGE - PIERCING       1.00      1.00      1.00        14\n",
      "                          TELEPHONIE - GPS       0.93      0.98      0.95      2183\n",
      "                     TENUE PROFESSIONNELLE       0.00      0.00      0.00         1\n",
      "                          TV - VIDEO - SON       0.83      0.31      0.45       109\n",
      "                     VETEMENTS - LINGERIE        0.77      0.84      0.81       618\n",
      "                   VIN - ALCOOL - LIQUIDES       0.93      0.52      0.67        25\n",
      "\n",
      "                                  accuracy                           0.81     10000\n",
      "                                 macro avg       0.64      0.50      0.54     10000\n",
      "                              weighted avg       0.81      0.81      0.80     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "y_train = data_train[\"Categorie1\"]\n",
    "y_test = data_valid[\"Categorie1\"]\n",
    "\n",
    "RandFores_clf = RandomForestClassifier(n_estimators=50,class_weight=\"balanced\",n_jobs=-1)\n",
    "RandFores_clf.fit(x_train, y_train)\n",
    "\n",
    "predicted = RandFores_clf.predict(x_val)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Boosting (AdaBoost)\n",
    "\n",
    "L’idée de base ressemble à celle du bagging. Plutôt que d’utiliser un seul modèle, nous en utilisons plusieurs que nous agrégeons ensuite pour obtenir un seul résultat. Dans  la construction des modèles, le Boosting travaille de manière séquentielle. Il commence par construire un premier modèle qu’il va évaluer. A partir de cette mesure, chaque individu va être pondéré en fonction de la performance de la prédiction. L’objectif est de donner un poids plus important aux individus pour lesquels la valeur a été mal prédite pour la construction du modèle suivant. Le fait de corriger les poids au fur et à mesure permet de mieux prédire les valeurs difficiles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/ada.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        ADULTE - EROTIQUE        0.44      0.24      0.32        98\n",
      "                          ANIMALERIE - NEW       0.47      0.34      0.40        64\n",
      "            ARME DE COMBAT - ARME DE SPORT       0.33      0.11      0.17         9\n",
      "     ART DE LA TABLE - ARTICLES CULINAIRES       0.41      0.21      0.28       111\n",
      "                      ARTICLES POUR FUMEUR       0.81      0.42      0.56        40\n",
      "                         AUTO - MOTO (NEW)       0.75      0.76      0.75       442\n",
      "                                 BAGAGERIE       0.81      0.78      0.80       215\n",
      "                   BATEAU MOTEUR - VOILIER       1.00      0.17      0.29         6\n",
      "              BIJOUX -  LUNETTES - MONTRES       0.89      0.93      0.91       591\n",
      "     BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.31      0.49      0.38       274\n",
      "                  CHAUSSURES - ACCESSOIRES       0.84      0.60      0.70       161\n",
      "                           CONDITIONNEMENT       0.00      0.00      0.00         2\n",
      "                            CULTURE / JEUX       0.75      0.76      0.75       160\n",
      "                  DECO - LINGE - LUMINAIRE       0.62      0.73      0.67       561\n",
      "                           DROGUERIE (NEW)       0.00      0.00      0.00        13\n",
      "                            ELECTROMENAGER       0.87      0.77      0.82       217\n",
      "                              ELECTRONIQUE       0.77      0.61      0.68        28\n",
      "                                  EPICERIE       0.14      0.04      0.06        25\n",
      "                 HYGIENE - BEAUTE - PARFUM       0.54      0.36      0.43       124\n",
      "                              INFORMATIQUE       0.92      0.91      0.91      1467\n",
      "                    INSTRUMENTS DE MUSIQUE       0.42      0.17      0.24        30\n",
      "                          JARDIN - PISCINE       0.59      0.36      0.45       153\n",
      "                               JOUET (NEW)       0.37      0.32      0.35       211\n",
      "                                 LIBRAIRIE       0.79      0.96      0.87      1194\n",
      "                                   LITERIE       0.79      0.58      0.67        38\n",
      " LOISIRS CREATIFS - BEAUX ARTS - PAPETERIE       0.42      0.25      0.31       100\n",
      "                               MANUTENTION       0.00      0.00      0.00         2\n",
      "                        MATERIEL DE BUREAU       0.00      0.00      0.00         6\n",
      "                          MATERIEL MEDICAL       0.00      0.00      0.00         4\n",
      "                                  MERCERIE       0.70      0.32      0.44        22\n",
      "                                   MEUBLE        0.66      0.55      0.60       136\n",
      "                             PARAPHARMACIE       0.07      0.03      0.05        30\n",
      "                           PHOTO - OPTIQUE       0.80      0.56      0.66       176\n",
      "POINT DE VENTE - COMMERCE - ADMINISTRATION       0.47      0.62      0.53        13\n",
      "                              PUERICULTURE       0.47      0.20      0.28        45\n",
      "                                 SONO - DJ       0.00      0.00      0.00        15\n",
      "                               SPORT (NEW)       0.40      0.36      0.38       267\n",
      "                       TATOUAGE - PIERCING       1.00      0.86      0.92        14\n",
      "                          TELEPHONIE - GPS       0.94      0.97      0.95      2183\n",
      "                     TENUE PROFESSIONNELLE       0.00      0.00      0.00         1\n",
      "                          TV - VIDEO - SON       0.67      0.32      0.43       109\n",
      "                     VETEMENTS - LINGERIE        0.65      0.76      0.70       618\n",
      "                   VIN - ALCOOL - LIQUIDES       1.00      0.12      0.21        25\n",
      "\n",
      "                                  accuracy                           0.77     10000\n",
      "                                 macro avg       0.53      0.41      0.44     10000\n",
      "                              weighted avg       0.76      0.77      0.76     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "y_train = data_train[\"Categorie1\"]\n",
    "y_test = data_valid[\"Categorie1\"]\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=40,algorithm=\"SAMME.R\",learning_rate=0.3)\n",
    "ada_clf.fit(x_train, y_train)\n",
    "\n",
    "predicted = ada_clf.predict(x_val)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### gradient boosting\n",
    "\n",
    "L’algorithme de boosting de gradient (gradient boosting) figure aussi parmi les méthodes de boosting très appréciées. Tout comme AdaBoost, le boosting de gradient travaille par ajout séquentiel de prédicteurs à un ensemble, chacun d’eux corrigeant son prédécesseur. Cependant, au lieu de modier légèrement les poids des observations à chaque itération comme le fait AdaBoost, cette méthode tente d’ajuster un nouveau prédicteur aux erreurs résiduelles du prédicteur précédent.\n",
    "\n",
    "Cet algorithme utilise le gradient de la fonction de perte pour le calcul des poids des individus lors de la construction de chaque nouveau modèle. Cela ressemble un peu à la descente de gradient pour les réseaux de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/Gradient_Boosting.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        ADULTE - EROTIQUE        0.57      0.48      0.52        98\n",
      "                          ANIMALERIE - NEW       0.53      0.33      0.40        64\n",
      "            ARME DE COMBAT - ARME DE SPORT       0.38      0.33      0.35         9\n",
      "     ART DE LA TABLE - ARTICLES CULINAIRES       0.61      0.48      0.54       111\n",
      "                      ARTICLES POUR FUMEUR       0.89      0.78      0.83        40\n",
      "                         AUTO - MOTO (NEW)       0.90      0.81      0.85       442\n",
      "                                 BAGAGERIE       0.86      0.83      0.84       215\n",
      "                   BATEAU MOTEUR - VOILIER       0.10      0.17      0.12         6\n",
      "              BIJOUX -  LUNETTES - MONTRES       0.94      0.92      0.93       591\n",
      "     BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.63      0.48      0.55       274\n",
      "                  CHAUSSURES - ACCESSOIRES       0.89      0.84      0.87       161\n",
      "                           CONDITIONNEMENT       0.00      0.00      0.00         2\n",
      "                            CULTURE / JEUX       0.82      0.72      0.77       160\n",
      "                  DECO - LINGE - LUMINAIRE       0.79      0.79      0.79       561\n",
      "                           DROGUERIE (NEW)       0.38      0.38      0.38        13\n",
      "                            ELECTROMENAGER       0.85      0.82      0.83       217\n",
      "                              ELECTRONIQUE       0.69      0.64      0.67        28\n",
      "                                  EPICERIE       0.33      0.16      0.22        25\n",
      "                 HYGIENE - BEAUTE - PARFUM       0.66      0.59      0.62       124\n",
      "                              INFORMATIQUE       0.79      0.92      0.85      1467\n",
      "                    INSTRUMENTS DE MUSIQUE       0.35      0.47      0.40        30\n",
      "                          JARDIN - PISCINE       0.67      0.58      0.62       153\n",
      "                               JOUET (NEW)       0.76      0.48      0.59       211\n",
      "                                 LIBRAIRIE       0.78      0.97      0.87      1194\n",
      "                                   LITERIE       0.71      0.84      0.77        38\n",
      " LOISIRS CREATIFS - BEAUX ARTS - PAPETERIE       0.55      0.48      0.51       100\n",
      "                               MANUTENTION       0.00      0.00      0.00         2\n",
      "                        MATERIEL DE BUREAU       0.33      0.33      0.33         6\n",
      "                          MATERIEL MEDICAL       0.33      0.25      0.29         4\n",
      "                                  MERCERIE       0.69      0.50      0.58        22\n",
      "                                   MEUBLE        0.82      0.74      0.78       136\n",
      "                             PARAPHARMACIE       0.27      0.13      0.18        30\n",
      "                           PHOTO - OPTIQUE       0.86      0.68      0.76       176\n",
      "POINT DE VENTE - COMMERCE - ADMINISTRATION       0.30      0.69      0.42        13\n",
      "                              PUERICULTURE       0.56      0.56      0.56        45\n",
      "                                 SONO - DJ       0.00      0.00      0.00        15\n",
      "                               SPORT (NEW)       0.81      0.45      0.58       267\n",
      "                       TATOUAGE - PIERCING       1.00      1.00      1.00        14\n",
      "                          TELEPHONIE - GPS       0.93      0.95      0.94      2183\n",
      "                     TENUE PROFESSIONNELLE       0.00      0.00      0.00         1\n",
      "                          TV - VIDEO - SON       0.67      0.47      0.55       109\n",
      "                     VETEMENTS - LINGERIE        0.82      0.81      0.81       618\n",
      "                   VIN - ALCOOL - LIQUIDES       0.83      0.60      0.70        25\n",
      "\n",
      "                                  accuracy                           0.82     10000\n",
      "                                 macro avg       0.60      0.55      0.56     10000\n",
      "                              weighted avg       0.82      0.82      0.81     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "y_train = data_train[\"Categorie1\"]\n",
    "y_test = data_valid[\"Categorie1\"]\n",
    "\n",
    "GradientBoostin_clf = GradientBoostingClassifier(n_estimators=40)\n",
    "GradientBoostin_clf.fit(x_train, y_train)\n",
    "\n",
    "predicted = GradientBoostin_clf.predict(x_val)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### régression logistique\n",
    "La régression logistique est utilisée couramment pour estimer la proba-bilité qu’une observation appartienne à une classe particulière (p. ex. quelle est la probabilité que cet e-mail soit un pourriel?). Si la probabilité estimée est supérieure à 50%, alors le modèle prédit que l’observation appartient à cette classe (appelée la classe positive, d’étiquette «1»), sinon il prédit qu’elle appartient à l’autre classe (la classe négative, d’étiquette «0»). C’est en fait un classificateur binaire.\n",
    "\n",
    "Tout comme un modèle de régression linéaire, un modèle de régression logistique calcule une somme pondérée des caractéristiques d’entrée (plus un terme constant), mais au lieu de fournir le résultat directement comme le fait le modèle de régression linéaire, il fournit la logistique du résultat.\n",
    "\n",
    "Le modèle de régression logistique peut être généralisé de manière à prendre en compte plusieurs classes directement, sans avoir à entraîner plusieurs classificateurs binaires puis à les combiner. C’est ce qu’on appelle la régression softmax, ou régression logistique multinomiale.\n",
    "Le principe en est simple: étant donné une observation x, le modèle de régression softmax calcule d’abord un score s_k(x) pour chaque classe k, puis estime la probabilité de chaque classe en appliquant aux scores la fonction softmax . La formule permettant de calculer s_k(x) devrait vous sembler familière, car c’est la même que pour calculer la prédiction en régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        ADULTE - EROTIQUE        0.57      0.72      0.64        98\n",
      "                          ANIMALERIE - NEW       0.55      0.48      0.52        64\n",
      "            ARME DE COMBAT - ARME DE SPORT       0.75      0.33      0.46         9\n",
      "     ART DE LA TABLE - ARTICLES CULINAIRES       0.59      0.74      0.66       111\n",
      "                      ARTICLES POUR FUMEUR       0.94      0.85      0.89        40\n",
      "                         AUTO - MOTO (NEW)       0.91      0.87      0.89       442\n",
      "                                 BAGAGERIE       0.89      0.94      0.92       215\n",
      "                   BATEAU MOTEUR - VOILIER       1.00      0.33      0.50         6\n",
      "              BIJOUX -  LUNETTES - MONTRES       0.96      0.96      0.96       591\n",
      "     BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.52      0.77      0.62       274\n",
      "                  CHAUSSURES - ACCESSOIRES       0.89      0.93      0.91       161\n",
      "                           CONDITIONNEMENT       0.00      0.00      0.00         2\n",
      "                            CULTURE / JEUX       0.80      0.85      0.82       160\n",
      "                  DECO - LINGE - LUMINAIRE       0.89      0.87      0.88       561\n",
      "                           DROGUERIE (NEW)       0.56      0.38      0.45        13\n",
      "                            ELECTROMENAGER       0.91      0.88      0.89       217\n",
      "                              ELECTRONIQUE       0.82      0.82      0.82        28\n",
      "                                  EPICERIE       0.38      0.32      0.35        25\n",
      "                 HYGIENE - BEAUTE - PARFUM       0.71      0.85      0.77       124\n",
      "                              INFORMATIQUE       0.97      0.90      0.94      1467\n",
      "                    INSTRUMENTS DE MUSIQUE       0.66      0.63      0.64        30\n",
      "                          JARDIN - PISCINE       0.75      0.70      0.73       153\n",
      "                               JOUET (NEW)       0.63      0.63      0.63       211\n",
      "                                 LIBRAIRIE       0.94      0.92      0.93      1194\n",
      "                                   LITERIE       0.85      0.92      0.89        38\n",
      " LOISIRS CREATIFS - BEAUX ARTS - PAPETERIE       0.67      0.70      0.69       100\n",
      "                               MANUTENTION       0.00      0.00      0.00         2\n",
      "                        MATERIEL DE BUREAU       0.50      0.17      0.25         6\n",
      "                          MATERIEL MEDICAL       0.00      0.00      0.00         4\n",
      "                                  MERCERIE       0.82      0.82      0.82        22\n",
      "                                   MEUBLE        0.87      0.93      0.90       136\n",
      "                             PARAPHARMACIE       0.64      0.23      0.34        30\n",
      "                           PHOTO - OPTIQUE       0.80      0.84      0.82       176\n",
      "POINT DE VENTE - COMMERCE - ADMINISTRATION       0.28      0.77      0.41        13\n",
      "                              PUERICULTURE       0.67      0.69      0.68        45\n",
      "                                 SONO - DJ       0.56      0.33      0.42        15\n",
      "                               SPORT (NEW)       0.65      0.68      0.66       267\n",
      "                       TATOUAGE - PIERCING       1.00      1.00      1.00        14\n",
      "                          TELEPHONIE - GPS       0.98      0.94      0.96      2183\n",
      "                     TENUE PROFESSIONNELLE       0.00      0.00      0.00         1\n",
      "                          TV - VIDEO - SON       0.52      0.79      0.63       109\n",
      "                     VETEMENTS - LINGERIE        0.90      0.89      0.89       618\n",
      "                   VIN - ALCOOL - LIQUIDES       0.89      0.64      0.74        25\n",
      "\n",
      "                                  accuracy                           0.87     10000\n",
      "                                 macro avg       0.68      0.65      0.65     10000\n",
      "                              weighted avg       0.88      0.87      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "y_train = data_train[\"Categorie1\"]\n",
    "y_test = data_valid[\"Categorie1\"]\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, class_weight=\"balanced\")\n",
    "softmax_reg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "predicted = softmax_reg.predict(x_val)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machines à vecteurs  de support\n",
    "\n",
    "Une machine à vecteurs de support ou séparateur à vaste marge (SVM) est un modèle d’apprentissage automatique à la fois puissant et polyvalent, capable d’effectuer des classifications linéaires ou non linéaires, des régressions et même des détections de données aberrantes. C’est un des modèles les plus prisés en matière d’apprentissage automatique, et tous ceux qui s’intéressent au Machine Learning devraient en avoir dans leur boîte à outils. Ils sont particulièrement bien adaptés à la classification de jeux de données complexes, de taille réduite ou moyenne.\n",
    "L'idée de Support Vector Machines est simple: l'algorithme crée une ligne qui sépare les classes, dans le cas par exemple d'un problème de classification. Le but de la ligne est de maximiser la marge entre les points de chaque côté de la ligne dite de décision. L'avantage de ce processus est qu'après la séparation, le modèle peut facilement deviner les classes cibles (étiquettes) pour les nouveaux cas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/svm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        ADULTE - EROTIQUE        0.59      0.74      0.66        98\n",
      "                          ANIMALERIE - NEW       0.46      0.48      0.47        64\n",
      "            ARME DE COMBAT - ARME DE SPORT       0.62      0.56      0.59         9\n",
      "     ART DE LA TABLE - ARTICLES CULINAIRES       0.60      0.73      0.66       111\n",
      "                      ARTICLES POUR FUMEUR       0.90      0.90      0.90        40\n",
      "                         AUTO - MOTO (NEW)       0.92      0.88      0.90       442\n",
      "                                 BAGAGERIE       0.87      0.94      0.90       215\n",
      "                   BATEAU MOTEUR - VOILIER       0.50      0.33      0.40         6\n",
      "              BIJOUX -  LUNETTES - MONTRES       0.96      0.97      0.97       591\n",
      "     BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.64      0.72      0.68       274\n",
      "                  CHAUSSURES - ACCESSOIRES       0.89      0.94      0.92       161\n",
      "                           CONDITIONNEMENT       1.00      0.50      0.67         2\n",
      "                            CULTURE / JEUX       0.82      0.82      0.82       160\n",
      "                  DECO - LINGE - LUMINAIRE       0.90      0.86      0.88       561\n",
      "                           DROGUERIE (NEW)       0.45      0.38      0.42        13\n",
      "                            ELECTROMENAGER       0.91      0.88      0.90       217\n",
      "                              ELECTRONIQUE       0.79      0.79      0.79        28\n",
      "                                  EPICERIE       0.38      0.40      0.39        25\n",
      "                 HYGIENE - BEAUTE - PARFUM       0.75      0.86      0.80       124\n",
      "                              INFORMATIQUE       0.98      0.92      0.95      1467\n",
      "                    INSTRUMENTS DE MUSIQUE       0.62      0.77      0.69        30\n",
      "                          JARDIN - PISCINE       0.78      0.73      0.75       153\n",
      "                               JOUET (NEW)       0.70      0.65      0.67       211\n",
      "                                 LIBRAIRIE       0.95      0.94      0.94      1194\n",
      "                                   LITERIE       0.84      0.95      0.89        38\n",
      " LOISIRS CREATIFS - BEAUX ARTS - PAPETERIE       0.66      0.70      0.68       100\n",
      "                               MANUTENTION       0.00      0.00      0.00         2\n",
      "                        MATERIEL DE BUREAU       0.33      0.17      0.22         6\n",
      "                          MATERIEL MEDICAL       1.00      0.25      0.40         4\n",
      "                                  MERCERIE       0.78      0.82      0.80        22\n",
      "                                   MEUBLE        0.81      0.95      0.87       136\n",
      "                             PARAPHARMACIE       0.45      0.30      0.36        30\n",
      "                           PHOTO - OPTIQUE       0.80      0.85      0.82       176\n",
      "POINT DE VENTE - COMMERCE - ADMINISTRATION       0.29      0.77      0.43        13\n",
      "                              PUERICULTURE       0.64      0.71      0.67        45\n",
      "                                 SONO - DJ       0.46      0.40      0.43        15\n",
      "                               SPORT (NEW)       0.69      0.70      0.69       267\n",
      "                       TATOUAGE - PIERCING       1.00      1.00      1.00        14\n",
      "                          TELEPHONIE - GPS       0.98      0.96      0.97      2183\n",
      "                     TENUE PROFESSIONNELLE       0.00      0.00      0.00         1\n",
      "                          TV - VIDEO - SON       0.59      0.78      0.67       109\n",
      "                     VETEMENTS - LINGERIE        0.89      0.90      0.89       618\n",
      "                   VIN - ALCOOL - LIQUIDES       0.85      0.68      0.76        25\n",
      "\n",
      "                                  accuracy                           0.88     10000\n",
      "                                 macro avg       0.70      0.69      0.68     10000\n",
      "                              weighted avg       0.89      0.88      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "y_train = data_train[\"Categorie1\"]\n",
    "y_test = data_valid[\"Categorie1\"]\n",
    "\n",
    "svm_clf = LinearSVC(class_weight=\"balanced\")\n",
    "svm_clf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "predicted = svm_clf.predict(x_val)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Naive Bayes Classifier\n",
    "La classification bayésienne naïve (Naive Bayes classifier), est utilisée depuis longtemps dans l'industrie et le monde universitaire (introduite par Thomas Bayes ), Cependant, cette technique est à l'étude depuis les années 50 pour la catégorisation des textes et des documents. \n",
    "Naive Bayes Classifier est un modèle génératif qui est largement utilisé dans la recherche d'informations, qui se base sur le théorème de Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/NB.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        ADULTE - EROTIQUE        0.00      0.00      0.00        98\n",
      "                          ANIMALERIE - NEW       1.00      0.03      0.06        64\n",
      "            ARME DE COMBAT - ARME DE SPORT       0.00      0.00      0.00         9\n",
      "     ART DE LA TABLE - ARTICLES CULINAIRES       1.00      0.04      0.07       111\n",
      "                      ARTICLES POUR FUMEUR       1.00      0.10      0.18        40\n",
      "                         AUTO - MOTO (NEW)       0.90      0.81      0.85       442\n",
      "                                 BAGAGERIE       0.96      0.52      0.67       215\n",
      "                   BATEAU MOTEUR - VOILIER       0.00      0.00      0.00         6\n",
      "              BIJOUX -  LUNETTES - MONTRES       0.76      0.94      0.84       591\n",
      "     BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.78      0.34      0.47       274\n",
      "                  CHAUSSURES - ACCESSOIRES       0.94      0.40      0.56       161\n",
      "                           CONDITIONNEMENT       0.00      0.00      0.00         2\n",
      "                            CULTURE / JEUX       0.98      0.62      0.76       160\n",
      "                  DECO - LINGE - LUMINAIRE       0.49      0.87      0.63       561\n",
      "                           DROGUERIE (NEW)       0.00      0.00      0.00        13\n",
      "                            ELECTROMENAGER       0.98      0.76      0.85       217\n",
      "                              ELECTRONIQUE       1.00      0.29      0.44        28\n",
      "                                  EPICERIE       0.00      0.00      0.00        25\n",
      "                 HYGIENE - BEAUTE - PARFUM       0.95      0.16      0.28       124\n",
      "                              INFORMATIQUE       0.79      0.91      0.84      1467\n",
      "                    INSTRUMENTS DE MUSIQUE       0.00      0.00      0.00        30\n",
      "                          JARDIN - PISCINE       1.00      0.16      0.27       153\n",
      "                               JOUET (NEW)       0.90      0.20      0.33       211\n",
      "                                 LIBRAIRIE       0.87      0.76      0.81      1194\n",
      "                                   LITERIE       1.00      0.18      0.31        38\n",
      " LOISIRS CREATIFS - BEAUX ARTS - PAPETERIE       1.00      0.03      0.06       100\n",
      "                               MANUTENTION       0.00      0.00      0.00         2\n",
      "                        MATERIEL DE BUREAU       0.00      0.00      0.00         6\n",
      "                          MATERIEL MEDICAL       0.00      0.00      0.00         4\n",
      "                                  MERCERIE       0.00      0.00      0.00        22\n",
      "                                   MEUBLE        1.00      0.35      0.51       136\n",
      "                             PARAPHARMACIE       0.00      0.00      0.00        30\n",
      "                           PHOTO - OPTIQUE       1.00      0.27      0.42       176\n",
      "POINT DE VENTE - COMMERCE - ADMINISTRATION       0.00      0.00      0.00        13\n",
      "                              PUERICULTURE       0.00      0.00      0.00        45\n",
      "                                 SONO - DJ       0.00      0.00      0.00        15\n",
      "                               SPORT (NEW)       0.88      0.19      0.32       267\n",
      "                       TATOUAGE - PIERCING       0.00      0.00      0.00        14\n",
      "                          TELEPHONIE - GPS       0.63      0.98      0.77      2183\n",
      "                     TENUE PROFESSIONNELLE       0.00      0.00      0.00         1\n",
      "                          TV - VIDEO - SON       1.00      0.07      0.14       109\n",
      "                     VETEMENTS - LINGERIE        0.64      0.93      0.76       618\n",
      "                   VIN - ALCOOL - LIQUIDES       0.00      0.00      0.00        25\n",
      "\n",
      "                                  accuracy                           0.72     10000\n",
      "                                 macro avg       0.52      0.25      0.28     10000\n",
      "                              weighted avg       0.75      0.72      0.67     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "y_train = data_train[\"Categorie1\"]\n",
    "y_test = data_valid[\"Categorie1\"]\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "predicted = NB_clf.predict(x_val)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### réseaux de neurones profond\n",
    "\n",
    "Les architectures des réseaux neuronaux profonds sont conçues pour apprendre à travers la connexion multiple de couches où chaque couche unique ne reçoit que la connexion de la précédente et fournit des connexions uniquement à la couche suivante dans la partie cachée. L'entrée est une connexion de l'espace d'entité (avec la première couche cachée). Pour les réseaux neuronaux profonds , la couche d'entrée peut être tf-ifd, one-hot encoding etc.., la couche de sortie contient des neurones égaux au nombre de classes pour la classification multi-classes, et un seul neurone pour la classification binaire.\n",
    "\n",
    "Lorsque nous n'avons que 2 classes (classification binaire), notre modèle devrait produire un seul score de probabilité. Par exemple, la sortie 0.2 pour un échantillon d'entrée donné signifie «20% de confiance que cet échantillon est dans la première classe (classe 1), 80% qu'il est dans la deuxième classe (classe 0)». Pour produire un tel score de probabilité, la fonction d'activation de la dernière couche doit être une fonction sigmoïde et la fonction de perte utilisée pour entraîner le modèle doit être une entropie croisée binaire (binary cross-entropy) .\n",
    "\n",
    "Lorsqu'il y a plus de 2 classes (classification multi-classes), notre modèle devrait produire un score de probabilité par classe. La somme de ces scores doit être 1. Par exemple, la sortie {0: 0.2, 1: 0.7, 2: 0.1}signifie «20% de confiance que cet échantillon est dans la classe 0, 70% qu'il est dans la classe 1 et 10% qu'il est dans la classe 2.» Pour produire ces scores, la fonction d'activation de la dernière couche doit être softmax et la fonction de perte utilisée pour entraîner le modèle doit être une entropie croisée catégorique(categorical cross-entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/RN.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y_train = data_train[[\"Categorie1\"]]\n",
    "y_test = data_valid[[\"Categorie1\"]]\n",
    "\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(y_train) \n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "train_labels = ordinal_encoder.fit_transform(y_train)\n",
    "val_labels = ordinal_encoder.transform(y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(max(val_labels)+1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "\n",
    "\n",
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation\n",
    "\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n",
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Get the data.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "data = ((data_train[\"Description\"], data_train[[\"Categorie1\"]]), (data_valid[\"Description\"], data_valid[[\"Categorie1\"]]))\n",
    "(train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "\n",
    "# Vectorize texts.\n",
    "x_train, x_val = ngram_vectorize(\n",
    "        train_texts, train_labels, val_texts)\n",
    "    \n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "train_labels = ordinal_encoder.fit_transform(train_labels)\n",
    "val_labels = ordinal_encoder.transform(val_labels)\n",
    "    \n",
    "num_classes = (max(val_labels)+1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def train_ngram_model(learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"    \n",
    "    # Create model instance.\n",
    "    model = mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=num_classes)\n",
    "    \n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    #model.save('IMDb_mlp_model.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      "40000/40000 - 17s - loss: 2.2191 - acc: 0.5762 - val_loss: 1.3085 - val_acc: 0.7256\n",
      "Epoch 2/1000\n",
      "40000/40000 - 18s - loss: 1.1026 - acc: 0.7596 - val_loss: 0.8862 - val_acc: 0.8056\n",
      "Epoch 3/1000\n",
      "40000/40000 - 18s - loss: 0.7858 - acc: 0.8231 - val_loss: 0.6958 - val_acc: 0.8431\n",
      "Epoch 4/1000\n",
      "40000/40000 - 17s - loss: 0.6075 - acc: 0.8602 - val_loss: 0.5920 - val_acc: 0.8611\n",
      "Epoch 5/1000\n",
      "40000/40000 - 17s - loss: 0.4892 - acc: 0.8862 - val_loss: 0.5277 - val_acc: 0.8747\n",
      "Epoch 6/1000\n",
      "40000/40000 - 17s - loss: 0.4089 - acc: 0.9048 - val_loss: 0.4881 - val_acc: 0.8801\n",
      "Epoch 7/1000\n",
      "40000/40000 - 17s - loss: 0.3509 - acc: 0.9169 - val_loss: 0.4651 - val_acc: 0.8840\n",
      "Epoch 8/1000\n",
      "40000/40000 - 17s - loss: 0.3105 - acc: 0.9259 - val_loss: 0.4516 - val_acc: 0.8866\n",
      "Epoch 9/1000\n",
      "40000/40000 - 17s - loss: 0.2733 - acc: 0.9349 - val_loss: 0.4428 - val_acc: 0.8873\n",
      "Epoch 10/1000\n",
      "40000/40000 - 17s - loss: 0.2467 - acc: 0.9386 - val_loss: 0.4348 - val_acc: 0.8886\n",
      "Epoch 11/1000\n",
      "40000/40000 - 17s - loss: 0.2225 - acc: 0.9455 - val_loss: 0.4322 - val_acc: 0.8901\n",
      "Epoch 12/1000\n",
      "40000/40000 - 18s - loss: 0.2026 - acc: 0.9500 - val_loss: 0.4303 - val_acc: 0.8894\n",
      "Epoch 13/1000\n",
      "40000/40000 - 17s - loss: 0.1891 - acc: 0.9532 - val_loss: 0.4319 - val_acc: 0.8909\n",
      "Epoch 14/1000\n",
      "40000/40000 - 17s - loss: 0.1736 - acc: 0.9558 - val_loss: 0.4327 - val_acc: 0.8898\n",
      "Validation accuracy: 0.8898000121116638, loss: 0.43267745552062986\n"
     ]
    }
   ],
   "source": [
    "model= train_ngram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        ADULTE - EROTIQUE        0.84      0.64      0.73        98\n",
      "                          ANIMALERIE - NEW       0.81      0.45      0.58        64\n",
      "            ARME DE COMBAT - ARME DE SPORT       1.00      0.11      0.20         9\n",
      "     ART DE LA TABLE - ARTICLES CULINAIRES       0.68      0.70      0.69       111\n",
      "                      ARTICLES POUR FUMEUR       1.00      0.72      0.84        40\n",
      "                         AUTO - MOTO (NEW)       0.89      0.90      0.89       442\n",
      "                                 BAGAGERIE       0.90      0.92      0.91       215\n",
      "                   BATEAU MOTEUR - VOILIER       0.00      0.00      0.00         6\n",
      "              BIJOUX -  LUNETTES - MONTRES       0.94      0.98      0.96       591\n",
      "     BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.60      0.74      0.66       274\n",
      "                  CHAUSSURES - ACCESSOIRES       0.90      0.91      0.91       161\n",
      "                           CONDITIONNEMENT       0.00      0.00      0.00         2\n",
      "                            CULTURE / JEUX       0.89      0.79      0.84       160\n",
      "                  DECO - LINGE - LUMINAIRE       0.79      0.90      0.84       561\n",
      "                           DROGUERIE (NEW)       0.67      0.15      0.25        13\n",
      "                            ELECTROMENAGER       0.93      0.89      0.91       217\n",
      "                              ELECTRONIQUE       0.91      0.71      0.80        28\n",
      "                                  EPICERIE       0.83      0.20      0.32        25\n",
      "                 HYGIENE - BEAUTE - PARFUM       0.82      0.82      0.82       124\n",
      "                              INFORMATIQUE       0.94      0.96      0.95      1467\n",
      "                    INSTRUMENTS DE MUSIQUE       0.80      0.53      0.64        30\n",
      "                          JARDIN - PISCINE       0.84      0.67      0.75       153\n",
      "                               JOUET (NEW)       0.73      0.67      0.70       211\n",
      "                                 LIBRAIRIE       0.93      0.96      0.94      1194\n",
      "                                   LITERIE       0.94      0.89      0.92        38\n",
      " LOISIRS CREATIFS - BEAUX ARTS - PAPETERIE       0.74      0.61      0.67       100\n",
      "                               MANUTENTION       0.00      0.00      0.00         2\n",
      "                        MATERIEL DE BUREAU       0.00      0.00      0.00         6\n",
      "                          MATERIEL MEDICAL       0.00      0.00      0.00         4\n",
      "                                  MERCERIE       0.94      0.73      0.82        22\n",
      "                                   MEUBLE        0.89      0.93      0.91       136\n",
      "                             PARAPHARMACIE       0.67      0.27      0.38        30\n",
      "                           PHOTO - OPTIQUE       0.94      0.84      0.88       176\n",
      "POINT DE VENTE - COMMERCE - ADMINISTRATION       0.67      0.31      0.42        13\n",
      "                              PUERICULTURE       0.76      0.56      0.64        45\n",
      "                                 SONO - DJ       1.00      0.13      0.24        15\n",
      "                               SPORT (NEW)       0.68      0.68      0.68       267\n",
      "                       TATOUAGE - PIERCING       1.00      1.00      1.00        14\n",
      "                          TELEPHONIE - GPS       0.96      0.98      0.97      2183\n",
      "                     TENUE PROFESSIONNELLE       0.00      0.00      0.00         1\n",
      "                          TV - VIDEO - SON       0.78      0.57      0.66       109\n",
      "                     VETEMENTS - LINGERIE        0.86      0.92      0.89       618\n",
      "                   VIN - ALCOOL - LIQUIDES       0.94      0.60      0.73        25\n",
      "\n",
      "                                  accuracy                           0.89     10000\n",
      "                                 macro avg       0.73      0.59      0.63     10000\n",
      "                              weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "predicted = model.predict_classes(x_val)\n",
    "predicted = pd.DataFrame(predicted)\n",
    "predicted = ordinal_encoder.inverse_transform(e_dataframe)\n",
    "print(metrics.classification_report(data_valid[\"Categorie1\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Étape 5:régler les hyperparamètres\n",
    "\n",
    "Nous avons dû choisir un certain nombre d'hyperparamètres pour définir et former le modèle. Nous nous sommes appuyés sur l'intuition, des exemples et des recommandations de bonnes pratiques. Cependant, notre premier choix de valeurs hyperparamétriques peut ne pas donner les meilleurs résultats. Cela ne nous donne qu'un bon point de départ pour la formation. Chaque problème est différent et le réglage de ces hyperparamètres aidera à affiner votre modèle. pour mieux représenter les particularités du problème à résoudre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://lovelyanalytics.com/\n",
    "https://spacy.io/\n",
    "https://developers.google.com/\n",
    "https://github.com/kk7nc/Text_Classification\n",
    "https://www.tensorflow.org/\n",
    "https://scikit-learn.org/stable/\n",
    "https://medium.com/@LSchultebraucks/introduction-to-support-vector-machines-9f8161ae2fcb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
