{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP (Natural Language Processing)-Machine learning Classification 4(Sequence Vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le dernier article, nous avons appris comment développer un classificateur de texte basé sur des vecteur n-gram, en utilisant des méthodes d'extraction de fonctionnalités . Nous avons également couvert différents algorithmes qui relèvent de l'apprentissage supervisé.\n",
    "\n",
    "Comme nous l'avons expliqué dans les articles précédents, Avec la représentation vectorielle n-gram, nous jetons beaucoup d'informations sur l'ordre des mots et la grammaire (au mieux, nous pouvons conserver certaines informations de classement partiel lorsque n> 1). C'est ce que l'on appelle une approche «bag of words». \n",
    "\n",
    "L’approche bag of words ne tient pas compte de l’ordre des mots, ni du contexte dans lequel ils interviennent, cette représentation est utilisée en conjonction avec des modèles qui ne tiennent pas compte de l'ordre, tels que la régression logistique, les perceptrons multicouches ...etc\n",
    "\n",
    "Pour certains exemples de texte, l'ordre des mots est essentiel à la signification du texte. Le texte est l'un des types de données séquentielles les plus couramment utilisés. Les données textuelles peuvent être vues soit comme une séquence de caractères, soit comme une séquence de mots. Il est courant de voir le texte comme une suite de mots pour la plupart des problèmes. Les modèles séquentiels d'apprentissage en profondeur tels que les RNNS et ses variantes peuvent apprendre des modèles importants à partir de données textuelles qui peuvent résoudre des problèmes dans nombreux domaines tels que ,\n",
    "compréhension du langage naturel, Classification des documents, analyse des sentiments, traduction ...etc\n",
    "\n",
    "bien que les données de séries temporelles, la musique, le son et autres soient également considérées comme des données séquentielles. Le traitement du langage naturel (NLP) et sa compréhension ont été largement explorés et c'est un domaine de recherche actif en ce moment. Le langage humain est incroyablement complexe et les combinaisons possibles de tout notre vocabulaire sont plus que le nombre d'atomes dans l'univers. Cependant, les réseaux profonds gèrent assez bien ce problème en utilisant certaines techniques comme les embeddings et l'attention.\n",
    "\n",
    "Nous avons deja vu, comment effectuer la tokenisation et la vectorisation pour les modèles de séquence. Nous avons vu  également comment optimiser la représentation des séquences en utilisant des techniques de sélection et de normalisation des caractéristiques.\n",
    "Dans cet aticle , nous allons examiner les données séquentielles. et continue à construire notre modèle de séquence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4: Créez, formez et évaluez votre modèle\n",
    "À l' étape 3 , nous avons choisi d'utiliser un modèle à n-grammes ou un modèle de séquence. Maintenant, il est temps d'écrire notre algorithme de classification de sequence  et de le former\n",
    "##### Construire un modèle de séquence [Option B]\n",
    "Nous appelons modèles de séquence les modèles qui peuvent apprendre de l'adjacence des jetons. Cela inclut les classes de modèles CNN et RNN. Les données sont prétraitées en tant que vecteurs de séquence pour ces modèles.\n",
    "\n",
    "Les modèles séquentiels ont généralement un plus grand nombre de paramètres à apprendre. La première couche de ces modèles est une couche **d'embedding**, qui apprend la relation entre les mots dans un espace vectoriel. L'apprentissage des relations entre les mots fonctionne mieux sur de nombreux exemples.\n",
    "\n",
    "Les mots d'un ensemble de données ne sont probablement pas uniques. Nous pouvons donc apprendre la relation entre les mots de notre ensemble de données en utilisant d'autres ensembles de données. Pour ce faire, nous pouvons transférer des embeddings apprise à partir d'un autre ensemble de données dans notre couche d'embedding. Ces embeddings sont appelés embeddings pré-formés . L'utilisation des embeddings pré-formées donne au modèle une longueur d'avance dans le processus d'apprentissage.\n",
    "\n",
    "\n",
    "Il existe des embeddings pré-formés disponibles statique et denamique qui ont été formés sur plusieurs corpus (principalement Wikipedia).\n",
    "\n",
    "\n",
    "\n",
    "![](data/embed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static Word Embedding\n",
    "\n",
    "* Skip-Gram & CBOW (aka Word2Vec)\n",
    "* Glove\n",
    "* fastText\n",
    "* Exotic: Lda2Vec, Node2Vec, Characters Embeddings, CNN embeddings, …\n",
    "* Poincaré Embeddings to learn hierarchical representation\n",
    "\n",
    "...\n",
    "\n",
    "#### Contextualized (Dynamic) Word Embedding (LM)\n",
    "* CoVe (Contextualized Word-Embeddings)\n",
    "* CVT (Cross-View Training)\n",
    "* ELMO (Embeddings from Language Models)\n",
    "* ULMFiT (Universal Language Model Fine-tuning)\n",
    "* BERT (Bidirectional Encoder Representations from Transformers)\n",
    "* GPT & GPT-2 (Generative Pre-Training)\n",
    "* Transformer XL (meaning extra long)\n",
    "* XLNet (Generalized Autoregressive Pre-training)\n",
    "* RoBERTa \n",
    "* CamemBERT \n",
    "* ENRIE (Enhanced Representation through kNowledge IntEgration)\n",
    "(FlairEmbeddings (Contextual String Embeddings for Sequence Labelling))\n",
    "\n",
    "et bien d'autres encore…\n",
    "\n",
    "En  gros les Embeddings de mots statiques ne parviennent pas à capturer la polysémie . Ils génèrent la même intégration pour le même mot dans des contextes différents . et  les Embeddings de mots contextualisés (Dynamic) visent à capturer la sémantique des mots dans différents contextes pour résoudre le problème de la polysémie et de la nature contextuelle des mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons testé l'entraînement de nos modèles de séquence à l'aide d'une version des intégrations FastText(francais) et nous  avons observé que si nous gelions les poids des intégrations pré-formées et entraînant uniquement le reste du réseau, les modèles ne fonctionnaient pas bien. Cela peut être dû au fait que le contexte dans lequel la couche d'enbedding a été formée peut avoir été différent du contexte dans lequel nous l'utilisions.\n",
    "\n",
    "Les embeddings FastText formées sur les données Wikipédia peuvent ne pas s'aligner sur les modèles linguistiques de notre ensemble de données. Les relations inférées peuvent nécessiter une mise à jour, c'est-à-dire que les poids d'intégration peuvent nécessiter un ajustement contextuel. Nous le faisons en deux étapes:\n",
    "\n",
    "\n",
    "* Dans un premier temps, les poids de la couche embedding étant gelés, nous permettons au reste du réseau d'apprendre. À la fin de cette exécution, les poids du modèle atteignent un état bien meilleur que leurs valeurs non initialisées. Pour la deuxième exécution, nous permettons à la couche d'embeddings d'apprendre également, en ajustant tous les poids du réseau.Nous appelons ce processus fine-tuning..\n",
    "\n",
    "\n",
    "\n",
    "* Des embeddings affinés(fine-tuning) donnent une meilleure précision. Cependant, cela se fait au détriment de la puissance de calcul nécessaire pour entraîner le réseau. Étant donné un nombre suffisant d'échantillons, nous pourrions tout aussi bien apprendre des embeddings à partir de zéro  donne effectivement à peu près la même précision que l'utilisation des embeddings affinés.\n",
    "\n",
    "Nous avons comparé différents modèles de séquence tels que CNN,RNN (LSTM & GRU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  formation des  embeddings et construisant notre classificateur\n",
    "le processus de construction\n",
    "* construisons notre classificateur de produits  et  nous formerons également des embeddings des mots présents dans l'ensemble de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utiliserons une bibliothèque appelée torchtext, qui facilite beaucoup le processus la vectorisation de texte et le traitement par lots. \n",
    "\n",
    "La formation d'un classificateur de text comprendra les étapes suivantes:\n",
    "\n",
    "* télécharger des données  et effectuer une tokenisation de texte \n",
    "* 1. construire un vocabulaire \n",
    "* 2. générer des lots de vecteurs \n",
    "* 3. créer un modèle de réseau avec des embeddings \n",
    "* 4. former le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Télécharger des données et effectuer une tokenisation de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as sms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(input_path, nb_line, tauxValid):\n",
    "    data_all = pd.read_csv(input_path,sep=\",\", nrows=nb_line)\n",
    "    data_all = data_all[[\"Description\",\"Categorie1\"]]\n",
    "    data_all.rename(columns={\"Description\": \"text\", \"Categorie1\": \"label\"}, inplace = True)\n",
    "    data_all = data_all.fillna(\"\")\n",
    "    data_train, data_valid = sms.train_test_split(data_all, test_size = tauxValid,random_state=47)\n",
    "    return data_train, data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set : 8000 elements, Validation set : 2000 elements\n"
     ]
    }
   ],
   "source": [
    "input_path = \"data/cdiscount_train.csv.zip\"\n",
    "nb_line=10000 # part totale extraite du fichier initial ici déjà réduit\n",
    "tauxValid = 0.2\n",
    "data_train, data_valid = split_dataset(input_path, nb_line, tauxValid)\n",
    "#data_train.reset_index(inplace = True)\n",
    "#data_valid.reset_index(inplace = True)\n",
    "N_train = data_train.shape[0]\n",
    "N_valid = data_valid.shape[0]\n",
    "print(\"Train set : %d elements, Validation set : %d elements\" %(N_train, N_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "train_labels = ordinal_encoder.fit_transform(data_train[[\"label\"]])\n",
    "val_labels = ordinal_encoder.transform(data_valid[[\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[[\"label\"]] = train_labels\n",
    "data_valid[[\"label\"]] = val_labels\n",
    "\n",
    "#converting dtypes using astype \n",
    "data_train[\"label\"]= data_train[\"label\"].astype(int) \n",
    "data_valid[\"label\"]= data_valid[\"label\"].astype(int)\n",
    "\n",
    "data_train[\"text\"]= data_train[\"text\"].astype(str) \n",
    "data_valid[\"text\"]= data_valid[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'instance torchtext.data définit une classe Field, qui nous aide à définir comment les données doivent être lues et tokenisées. Regardons l'exemple suivant, que nous utiliserons pour préparer notre jeu de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from spacy.lang.fr import French\n",
    "from sklearn.feature_extraction.text import strip_accents_ascii\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "config = {'embed_size': 300,\n",
    " 'output_size': int(max(train_labels)[0])+1,\n",
    " 'lr': 0.001,\n",
    " 'batch_size': 128,\n",
    " 'max_sen_len': 30,\n",
    " 'dropout_keep': 0.3,\n",
    " 'max_epochs': 5}\n",
    "\n",
    "\n",
    "\n",
    "NLP = French()\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = NLP(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [word.lemma_.lower() for word in mytokens if word.text != \" \" and\n",
    "                not word.is_punct and not word.like_num and word.text != 'n']\n",
    "    # Removing stop words\n",
    "    #mytokens = [word for word in mytokens if word not in STOP_WORDS]\n",
    "\n",
    "    # Remove accentuated char for any unicode symbol\n",
    "    mytokens = [strip_accents_ascii(word) for word in mytokens]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "#tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != \" \" and not x.is_punct and not x.like_num and x.text != 'n']      \n",
    "# Creating Field for data\n",
    "TEXT = data.Field(sequential=True,batch_first=True,tokenize=tokenizer,lower=True, fix_length=config[\"max_sen_len\"])\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "datafields = [(\"text\",TEXT),(\"label\",LABEL)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code précédent, nous définissons deux objets Field, un pour le texte réel et un autre pour les données d'étiquette. Pour le texte réel, nous nous attendons à ce que torchtext réduise en minuscules tout le texte, le coupe à une longueur maximale de 30. Si nous créons une application pour un environnement de production, nous pouvons fixer la longueur à un nombre beaucoup plus grand. Le constructeur Field accepte également un autre argument appelé tokenize, qui utilise par défaut la fonction str.split. Nous pouvons également spécifier spaCy comme argument, ou tout autre tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### Construire du vocabulaire\n",
    "\n",
    " L'instance torchtext nous facilite la tâche. Une fois les données chargées, nous pouvons appeler build_vocab et passer les arguments nécessaires qui prendront soin de construire le vocabulaire des données. Le code suivant montre comment le vocabulaire est construit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load data from pd.DataFrame into torchtext.data.Dataset\n",
    "train_examples = [data.Example.fromlist(i, datafields) for i in data_train.values.tolist()]\n",
    "train_data = data.Dataset(train_examples, datafields)\n",
    "\n",
    "val_examples = [data.Example.fromlist(i, datafields) for i in data_valid.values.tolist()]\n",
    "data_valid = data.Dataset(val_examples, datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "TEXT.build_vocab(train_data, vectors=Vectors('C:\\DEV\\Article\\data\\cc.fr.300.vec'),max_size=20000,min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code précédent, nous passons l'objet train_data sur lequel nous devons construire le vocabulaire, et nous lui demandons également d'initialiser des vecteurs avec des embeddings pré-formées de dimensions 300(faster text fr). L'objet build_vocab télécharge et crée la dimension qui sera utilisée plus tard lorsque nous entraînerons norte modèle. L'instance max_size limite le nombre de mots dans le vocabulaire et min_freq supprime tout mot qui ne s'est pas produit plus de 2 fois.\n",
    "\n",
    "Une fois le vocabulaire construit, nous pouvons obtenir différentes valeurs telles que la fréquence, l'index des mots et la représentation vectorielle pour chaque mot. Le code suivant montre comment accéder à ces valeurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(TEXT.vocab.freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0012, -0.0920,  0.0766,  ...,  0.1596, -0.0015,  0.0556],\n",
      "        ...,\n",
      "        [-0.0690, -0.0271,  0.0054,  ..., -0.0206,  0.0860, -0.0181],\n",
      "        [-0.0205, -0.0975, -0.1952,  ..., -0.0664, -0.0520,  0.2363],\n",
      "        [-0.0777, -0.0255,  0.0045,  ..., -0.0925,  0.0635,  0.0650]])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même, nous imprimerons les valeurs d'un dictionnaire contenant des mots et leurs index.\n",
    "La valeur stoi donne accès à un dictionnaire contenant des mots et leurs index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = TEXT.vocab.vectors\n",
    "vocab = TEXT.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Génération des lots de vecteurs\n",
    "\n",
    "torchtext fournit BucketIterator, qui aide à regrouper tout le texte et à remplacer les mots par le numéro d'index des mots. L'instance BucketIterator est livrée avec de nombreux paramètres utiles tels que batch_size,et shuffle (si les données doivent être mélangées). Le code suivant montre comment créer des itérateurs qui génèrent des lots ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator,val_iterator = data.BucketIterator.splits(\n",
    "            (train_data,data_valid),\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Création d'un modèle de réseau avec embeddings\n",
    "\n",
    "Nous créons des embeddings de mots dans le cadre de notre architecture réseau et formons l'ensemble du modèle pour prédire la catégorie de chaque produit. À la fin de l'entraînement , nous aurons un modèle de classification et également des embeddings de mots pour notre jeux données . Le code suivant montre comment créer une architecture réseau pour prédire la categorie d'un produit en utilisant des embaddings de mots.\n",
    "\n",
    "La variable vectors renvoie un tenseur de forme vocab_size x dimensions contenant les embedings pré-entraînés. Nous devons stocker les embeddings dans les poids de notre couche d'embedding. Nous pouvons attribuer les poids des embeddings en accédant aux poids de la couche des embedding.\n",
    "\n",
    "Une fois les embeddings chargés,  \n",
    "\n",
    "* soit on  gele les poids de la couche d'embeddings et nous permettons au reste du réseau d'apprendre.\n",
    "* soit nous permettons à la couche d'embedding d'apprendre également, en ajustant tous les poids du réseau\n",
    "\n",
    "pour indiquer à PyTorch de ne pas modifier les poids de la couche d'intégration ou si on veut le contraire, définissez l'attribut requires_grad sur False resp(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self,config,emb_size,word_embeddings): \n",
    "        super(EmbeddingNetwork, self).__init__() \n",
    "        \n",
    "        self.embedding = nn.Embedding(emb_size,config[\"embed_size\"])\n",
    "        self.embedding.weight = nn.Parameter(word_embeddings, requires_grad=True)\n",
    "        self.fc = nn.Linear(config[\"embed_size\"]*config[\"max_sen_len\"],64) \n",
    "        self.fc2 = nn.Linear(64,config[\"output_size\"]) \n",
    "        self.dropout = nn.Dropout(config[\"dropout_keep\"])\n",
    "      \n",
    "        \n",
    "\n",
    "    def forward(self,x):        \n",
    "        embeds = self.embedding(x).view(x.size(0),-1)        \n",
    "        out = self.fc(self.dropout(embeds))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return F.log_softmax(out,dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code précédent, EmbeddingNetwork crée le modèle de classification. A l'intérieur de la fonction _init_, nous initialisons un objet de la classe nn.Embedding, qui prend deux arguments, à savoir la taille du vocabulaire et les dimensions que nous souhaitons créer pour chaque mot. Comme nous avons limité le nombre de mots uniques, la taille du vocabulaire sera de 20 000. Nous avons également une couche linéaire qui mappe les embeddings de mots ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Entraînement du modèle\n",
    "\n",
    "Maintenant que nous avons construit l'architecture du modèle, nous devons entraîner le modèle. L'entraînement consiste de faire une prédiction basée sur l'état actuel du modèle,  à calculer le degré d'erreur de la prédiction et à mettre à jour les poids ou les paramètres du réseau pour minimiser cette erreur et améliorer la prédiction du modèle. Nous répétons ce processus jusqu'à ce que notre modèle ait convergé et ne puisse plus apprendre. Trois paramètres clés doivent être choisis pour ce processus .\n",
    "\n",
    "* **Métrique** : Comment mesurer la performance de notre modèle à l'aide d'une métrique. Nous avons utilisé l'accuracy  comme métrique dans nos expériences.\n",
    "\n",
    "* **Fonction de perte** : fonction utilisée pour calculer une valeur de perte que le processus de formation tente ensuite de minimiser en réglant les poids du réseau. Pour les problèmes de classification, la perte d'entropie croisée fonctionne bien.\n",
    "\n",
    "* **Optimiseur** : Une fonction qui décide comment les poids du réseau seront mis à jour en fonction de la sortie de la fonction de perte. Nous avons utilisé le populaire optimiseur Adam dans nos expériences.\n",
    "\n",
    "\n",
    "\n",
    "###### Paramètres d'apprentissage\n",
    "\n",
    "\n",
    "L'entraînement proprement se fait en utilisant une méthode d'ajustement . Selon la taille de votre ensemble de données, il s'agit de la méthode dans laquelle la plupart des cycles de calcul seront passés. Dans chaque itération d'entraînement, le batch_size  d'échantillons de vos données d'entraînement est utilisé pour calculer la perte et les poids sont mis à jour une fois, en fonction de cette valeur. Le processus de d'entraînement se termine une fois que le modèle a vu l'ensemble de données de d'entraînement complet. À la fin de chaque époque, nous utilisons l'ensemble de données de validation pour évaluer dans quelle mesure le modèle apprend. Nous répétons la formation en utilisant l'ensemble de données pour un nombre prédéterminé d'époques. Nous pouvons optimiser cela en s'arrêtant tôt, lorsque la précision de validation se stabilise entre les époques consécutives, montrant que le modèle ne s'entraîne plus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def evaluate_model(model, iterator):\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for idx,batch in enumerate(iterator):\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "        else:\n",
    "            x = batch.text\n",
    "        y_pred = model(x)\n",
    "        predicted = torch.max(y_pred.cpu().data, 1)[1]\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_y.extend(batch.label.numpy())\n",
    "    score = accuracy_score(all_y, np.array(all_preds).flatten())\n",
    "    return score\n",
    "def run_epoch(model, train_iterator, val_iterator, epoch):\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    losses = []\n",
    "\n",
    "\n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        optimizer.zero_grad()\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "            y = (batch.label).type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            x = batch.text\n",
    "            y = (batch.label).type(torch.LongTensor)\n",
    "        y_pred = model.__call__(x)\n",
    "        loss = loss_op(y_pred, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.data.cpu().numpy())\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Iter: {}\".format(i+1))\n",
    "            avg_train_loss = np.mean(losses)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
    "            losses = []\n",
    "\n",
    "            # Evalute Accuracy on validation set\n",
    "            val_accuracy = evaluate_model(model,val_iterator)\n",
    "            print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
    "            model.train()\n",
    "\n",
    "    return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 1\n",
      "\tAverage training loss: 3.76587\n",
      "\tVal Accuracy: 0.2218\n",
      "Epoch: 1\n",
      "Iter: 1\n",
      "\tAverage training loss: 0.32898\n",
      "\tVal Accuracy: 0.8672\n",
      "Epoch: 2\n",
      "Iter: 1\n",
      "\tAverage training loss: 0.14793\n",
      "\tVal Accuracy: 0.8878\n",
      "Epoch: 3\n",
      "Iter: 1\n",
      "\tAverage training loss: 0.09497\n",
      "\tVal Accuracy: 0.8909\n",
      "Epoch: 4\n",
      "Iter: 1\n",
      "\tAverage training loss: 0.07355\n",
      "\tVal Accuracy: 0.8907\n",
      "Final Training Accuracy: 0.9939\n",
      "Final Validation Accuracy: 0.8924\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = EmbeddingNetwork(config, len(vocab),word_embeddings)\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "loss_op = F.nll_loss\n",
    "\n",
    "for i in range(config[\"max_epochs\"]):\n",
    "    print (\"Epoch: {}\".format(i))\n",
    "    train_loss,val_accuracy = run_epoch(model,train_iterator, val_iterator, i)\n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "train_acc = evaluate_model(model, train_iterator)\n",
    "val_acc = evaluate_model(model, val_iterator)\n",
    "#test_acc = evaluate_model(model, dataset.test_iterator)\n",
    "\n",
    "print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
    "print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n",
    "#print ('Final Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code précédent, nous appelons la méthode run_epoch en passant l'objet BucketIterator que nous avons créé pour regrouper les données. La formation du modèle sur environ 5 époques donne une précision de validation d'environ 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "predicted = model.predict_classes(x_val)\n",
    "predicted = pd.DataFrame(predicted)\n",
    "predicted = ordinal_encoder.inverse_transform(e_dataframe)\n",
    "print(metrics.classification_report(data_valid[\"Categorie1\"], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iterator\n",
    "all_preds = []\n",
    "all_y = []\n",
    "for idx,batch in enumerate(val_iterator):\n",
    "    if torch.cuda.is_available():\n",
    "        x = batch.text.cuda()\n",
    "    else:\n",
    "        x = batch.text\n",
    "    y_pred = model(x)\n",
    "    predicted = torch.max(y_pred.cpu().data, 1)[1]\n",
    "    all_preds.extend(predicted.numpy())\n",
    "    all_y.extend(batch.label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        ADULTE - EROTIQUE        0.67      0.58      0.62       175\n",
      "                          ANIMALERIE - NEW       0.61      0.51      0.56       129\n",
      "            ARME DE COMBAT - ARME DE SPORT       0.12      0.07      0.09        14\n",
      "     ART DE LA TABLE - ARTICLES CULINAIRES       0.63      0.72      0.67       196\n",
      "                      ARTICLES POUR FUMEUR       0.89      0.68      0.77        71\n",
      "                         AUTO - MOTO (NEW)       0.92      0.89      0.90       968\n",
      "                                 BAGAGERIE       0.93      0.91      0.92       425\n",
      "                   BATEAU MOTEUR - VOILIER       0.38      0.14      0.21        21\n",
      "              BIJOUX -  LUNETTES - MONTRES       0.95      0.98      0.96      1220\n",
      "     BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.74      0.79      0.76       628\n",
      "                  CHAUSSURES - ACCESSOIRES       0.91      0.88      0.90       287\n",
      "                           CONDITIONNEMENT       0.00      0.00      0.00         4\n",
      "                            CULTURE / JEUX       0.88      0.83      0.86       384\n",
      "                  DECO - LINGE - LUMINAIRE       0.86      0.88      0.87      1206\n",
      "                           DROGUERIE (NEW)       0.53      0.32      0.40        25\n",
      "                            ELECTROMENAGER       0.91      0.91      0.91       411\n",
      "                              ELECTRONIQUE       0.96      0.69      0.81        72\n",
      "                                  EPICERIE       0.53      0.56      0.55        41\n",
      "                 HYGIENE - BEAUTE - PARFUM       0.83      0.79      0.81       277\n",
      "                              INFORMATIQUE       0.96      0.96      0.96      2873\n",
      "                    INSTRUMENTS DE MUSIQUE       0.66      0.49      0.57        79\n",
      "                          JARDIN - PISCINE       0.68      0.67      0.67       270\n",
      "                               JOUET (NEW)       0.76      0.68      0.72       438\n",
      "                                 LIBRAIRIE       0.93      0.99      0.96      2357\n",
      "                                   LITERIE       0.90      0.83      0.86        88\n",
      " LOISIRS CREATIFS - BEAUX ARTS - PAPETERIE       0.73      0.61      0.66       180\n",
      "                               MANUTENTION       0.00      0.00      0.00         3\n",
      "                        MATERIEL DE BUREAU       0.20      0.10      0.13        20\n",
      "                          MATERIEL MEDICAL       0.00      0.00      0.00         7\n",
      "                                  MERCERIE       0.82      0.77      0.80        61\n",
      "                                   MEUBLE        0.85      0.85      0.85       282\n",
      "                             PARAPHARMACIE       0.71      0.57      0.63        56\n",
      "                           PHOTO - OPTIQUE       0.89      0.85      0.87       325\n",
      "POINT DE VENTE - COMMERCE - ADMINISTRATION       0.61      0.51      0.56        39\n",
      "                            PRODUITS FRAIS       0.00      0.00      0.00         2\n",
      "                              PUERICULTURE       0.57      0.58      0.57        99\n",
      "                                 SONO - DJ       0.54      0.36      0.43        36\n",
      "                               SPORT (NEW)       0.74      0.72      0.73       554\n",
      "                       TATOUAGE - PIERCING       0.96      0.90      0.93        29\n",
      "                          TELEPHONIE - GPS       0.97      0.98      0.98      4155\n",
      "                     TENUE PROFESSIONNELLE       0.67      0.33      0.44         6\n",
      "                          TV - VIDEO - SON       0.59      0.61      0.60       205\n",
      "                     VETEMENTS - LINGERIE        0.88      0.92      0.90      1251\n",
      "                   VIN - ALCOOL - LIQUIDES       0.74      0.74      0.74        31\n",
      "\n",
      "                                  accuracy                           0.89     20000\n",
      "                                 macro avg       0.67      0.62      0.64     20000\n",
      "                              weighted avg       0.89      0.89      0.89     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "predicted_df = pd.DataFrame(all_preds)\n",
    "all_y_df = pd.DataFrame(all_y)\n",
    "all_preds = ordinal_encoder.inverse_transform(predicted_df)\n",
    "all_y = ordinal_encoder.inverse_transform(all_y_df)\n",
    "print(metrics.classification_report(all_y,all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons former le modèle en utilisant ce code exact et devrions atteindre une précision similaire.\n",
    "Toutes les architectures de modèle ne parviennent pas à tirer parti de la nature séquentielle du texte.\n",
    "Dans la section suivante, nous explorons deux techniques populaires, à savoir RNN et Conv1D, qui profitent de la nature séquentielle des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réseau de neurones récurrents \n",
    "\n",
    "\n",
    "Les RNN sont parmi les modèles les plus puissants qui nous permettent de prendre en charge des applications telles que la classification, l'étiquetage des données séquentielles, la génération de séquences de texte et la conversion d'une séquence en une autre, comme lorsque traduire une langue (par exemple, du français vers l'anglais).\n",
    "\n",
    "Dans la plupart des langues modernes, les humains donnent un sens aux données textuelles en lisant les mots de gauche à droite et en construisant un modèle puissant qui comprend en quelque sorte toutes les différentes choses que dit le texte. RNN fonctionne de manière similaire en regardant un mot dans le texte à la fois. RNN est également un réseau aneural qui a une couche spéciale en elle, qui boucle sur les données au lieu de traiter tout à la fois. Comme les RNN peuvent traiter des données en séquence, nous pouvons utiliser des vecteurs de différentes longueurs et générer des sorties de différentes longueurs. Certaines des différentes représentations sont fournies dans le diagramme suivant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"data/RNN.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNNs\n",
    "\n",
    "\n",
    "c'est la version la plus simple des RNN proposée par Jeff Elman en 1990 et dans laquelle on rajoute des liens à un MLP pour donner en entrée d’une couche du réseau sa propre sortie au pas de temps précédent en plus de la sortie courante de la couche précédente. Dans le cas où les vecteurs d’entrées x sont indépendants les uns des autres cela n’a pas beaucoup d’intérêt mais dans le cas où l’on a des entrées sous la forme de séquences temporelles ou spatiales cette modification a un très grand impact. En effet, la structure d’un RNN introduit un mécanisme de mémoire des entrées précédentes qui persiste dans les états internes du réseau et peut ainsi impacter toutes ses sorties futures. Avec cette simple modification il est en théorie\n",
    "possible d’approximer n’importe quelle fonction qui transforme une séquence d’entrée en une séquence de sortie donnée avec une précision arbitraire. Ce que ne permet pas le MLP. Le revers de la médaille est que ce type de RNN peut être particulièrement\n",
    "difficile à entraîner (bien que des développements récents améliorent cela)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"data/RNN_V.jpg\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN bidirectionnel\n",
    "\n",
    "Dans le cas de nombreuses tâches de classification de séquences, pour prendre une décision à un instant donné il est intéressant de connaître le passé et tout ou partie du futur de la séquence. Les RNN que nous avons décrits dans la section précédente traitent les séquences dans l’ordre temporel et n’ont donc pas accès à l’information future. \n",
    "\n",
    "Pour s’attaquer à ce problème, on peut rajouter de l’information future dans les données d’entrée (par exemple en ajoutant au vecteur courant les 10 prochains vecteurs d’entrée) ou introduire un délai entre un vecteur d’entrée et sa cible pour que le RNN ait le temps de traiter un peu d’information future avant d’avoir à prendre sa décision.\n",
    "\n",
    "Cependant, ces différentes approches rajoutent des contraintes sur l’apprentissage soit en augmentant le nombre de paramètres dans la couche d’entrée, soit en forçant le RNN à apprendre le délai choisi pour donner sa réponse au bon moment. Et dans\n",
    "les deux cas on ne résout pas pour autant le problème puisqu’on introduit des hyperparamètres (nombre de vecteurs d’entrées à agréger ou délai de réponse) qui peuvent s’avérer difficiles à régler. De plus, on ne supprime en aucun cas la dissymétrie entre le passé et le futur pour ce qui est du contexte exploitable. \n",
    "\n",
    "En 1997, Schuster et Paliwal ont introduit une solution élégante appelée réseau de neurones récurrent bidirectionnel (bidirectional Recurrent Neural Network -BiRNN) qui consiste à présenter chaque séquence à traiter à deux RNN de même type mais avec des paramètres différents :\n",
    "*  le premier traite la séquence dans l’ordre naturel t : 1 → tf \n",
    "*  le second traite la séquence dans l’ordre inverse t : tf → 1.\n",
    "\n",
    "Les deux séquences obtenues en sortie des deux RNN sont ensuite concaténées avant\n",
    "d’être mises en entrée d’un MLP qui pour chaque vecteur d’entrée initial produit alors\n",
    "une sortie (par exemple une classification) qui se base sur tout le contexte passé via\n",
    "la sortie du premier RNN et tout le contexte futur via la sortie du second. Il est ainsi\n",
    "possible d’exploiter pleinement les capacités des RNN sur l’ensemble de la séquence\n",
    "pour chaque pas de temps.\n",
    "\n",
    "un BiRNN est toujours préférable à l’utilisation d’un RNN unidirectionnel ayant le même nombre de paramètres . C’est pour cette raison que dans toutes les expérimentations réalisées durant cet\n",
    "article nous avons toujours utilisé des BiRNN que ce soit avec des couches récurrentes\n",
    "standard ou des couches LSTM . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"data/RNN_BI.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour éviter d'écrire beaucoup de code dans cet article je mettrai à disposition tous les modèles construits dans cette partie sur mon référentiel git \n",
    "\n",
    "Le lien https://github.com/rachikbilal/NLP_Text_Classification\n",
    "\n",
    "**si vous voulez exécuter le code dans jupyter comme je le fais maintenant, vous devez télécharger le projet et définir la variable d'environnement PYTHONPATH où se trouve le projet, cela vous permettra d'accéder au code dans jupyter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **architecte de notre modèle**\n",
    "<img src=\"data/ARCHI.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set : 80000 elements, Validation set : 20000 elements\n",
      "Loaded 80000 training examples\n",
      "Loaded 20000 validation examples\n"
     ]
    }
   ],
   "source": [
    "from Model_RNN.utils import evaluate_model,Dataset\n",
    "from Model_RNN.model import RNNClassifier\n",
    "from Model_RNN.config import Config\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "config = Config()\n",
    "train_file = \"data/cdiscount_train.csv.zip\"\n",
    "\n",
    "w2v_file = 'C:\\DEV\\Article\\data\\cc.fr.300.vec'\n",
    "\n",
    "dataset = Dataset(config)\n",
    "dataset.load_data(w2v_file, train_file)\n",
    "\n",
    "# Create Model with specified optimizer and loss function\n",
    "##############################################################\n",
    "model = RNNClassifier(config, len(dataset.vocab), dataset.word_embeddings)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=config.lr)\n",
    "\n",
    "#NLLLoss = nn.NLLLoss()\n",
    "CrossEntropyLoss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model.add_optimizer(optimizer)\n",
    "model.add_loss_op(CrossEntropyLoss)\n",
    "\n",
    "    ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 1\n",
      "\tAverage training loss: 0.21522\n",
      "\tVal Accuracy: 0.8904\n",
      "Iter: 101\n",
      "\tAverage training loss: 0.07544\n",
      "\tVal Accuracy: 0.8917\n",
      "Iter: 201\n",
      "\tAverage training loss: 0.08640\n",
      "\tVal Accuracy: 0.8905\n",
      "Iter: 301\n",
      "\tAverage training loss: 0.08488\n",
      "\tVal Accuracy: 0.8902\n",
      "Iter: 401\n",
      "\tAverage training loss: 0.09048\n",
      "\tVal Accuracy: 0.8890\n",
      "Iter: 501\n",
      "\tAverage training loss: 0.08903\n",
      "\tVal Accuracy: 0.8885\n",
      "Iter: 601\n",
      "\tAverage training loss: 0.08466\n",
      "\tVal Accuracy: 0.8873\n",
      "Iter: 701\n",
      "\tAverage training loss: 0.09635\n",
      "\tVal Accuracy: 0.8892\n",
      "Iter: 801\n",
      "\tAverage training loss: 0.10032\n",
      "\tVal Accuracy: 0.8878\n",
      "Iter: 901\n",
      "\tAverage training loss: 0.08308\n",
      "\tVal Accuracy: 0.8871\n",
      "Iter: 1001\n",
      "\tAverage training loss: 0.09169\n",
      "\tVal Accuracy: 0.8875\n",
      "Iter: 1101\n",
      "\tAverage training loss: 0.11044\n",
      "\tVal Accuracy: 0.8873\n",
      "Iter: 1201\n",
      "\tAverage training loss: 0.10630\n",
      "\tVal Accuracy: 0.8871\n",
      "Epoch: 1\n",
      "Reducing LR\n",
      "Iter: 1\n",
      "\tAverage training loss: 0.03653\n",
      "\tVal Accuracy: 0.8890\n",
      "Iter: 101\n",
      "\tAverage training loss: 0.06440\n",
      "\tVal Accuracy: 0.8917\n",
      "Iter: 201\n",
      "\tAverage training loss: 0.06855\n",
      "\tVal Accuracy: 0.8890\n",
      "Iter: 301\n",
      "\tAverage training loss: 0.06483\n",
      "\tVal Accuracy: 0.8900\n",
      "Iter: 401\n",
      "\tAverage training loss: 0.06742\n",
      "\tVal Accuracy: 0.8902\n",
      "Iter: 501\n",
      "\tAverage training loss: 0.06418\n",
      "\tVal Accuracy: 0.8895\n",
      "Iter: 601\n",
      "\tAverage training loss: 0.05897\n",
      "\tVal Accuracy: 0.8901\n",
      "Iter: 701\n",
      "\tAverage training loss: 0.06116\n",
      "\tVal Accuracy: 0.8919\n",
      "Iter: 801\n",
      "\tAverage training loss: 0.07405\n",
      "\tVal Accuracy: 0.8905\n",
      "Iter: 901\n",
      "\tAverage training loss: 0.05783\n",
      "\tVal Accuracy: 0.8902\n",
      "Iter: 1001\n",
      "\tAverage training loss: 0.06405\n",
      "\tVal Accuracy: 0.8911\n",
      "Iter: 1101\n",
      "\tAverage training loss: 0.06901\n",
      "\tVal Accuracy: 0.8921\n",
      "Iter: 1201\n",
      "\tAverage training loss: 0.06811\n",
      "\tVal Accuracy: 0.8904\n",
      "Epoch: 2\n",
      "Iter: 1\n",
      "\tAverage training loss: 0.01478\n",
      "\tVal Accuracy: 0.8917\n",
      "Iter: 101\n",
      "\tAverage training loss: 0.05106\n",
      "\tVal Accuracy: 0.8919\n",
      "Iter: 201\n",
      "\tAverage training loss: 0.05081\n",
      "\tVal Accuracy: 0.8918\n",
      "Iter: 301\n",
      "\tAverage training loss: 0.05344\n",
      "\tVal Accuracy: 0.8924\n",
      "Iter: 401\n",
      "\tAverage training loss: 0.05725\n",
      "\tVal Accuracy: 0.8904\n",
      "Iter: 501\n",
      "\tAverage training loss: 0.04790\n",
      "\tVal Accuracy: 0.8922\n",
      "Iter: 601\n",
      "\tAverage training loss: 0.05202\n",
      "\tVal Accuracy: 0.8925\n",
      "Iter: 701\n",
      "\tAverage training loss: 0.05309\n",
      "\tVal Accuracy: 0.8921\n",
      "Iter: 801\n",
      "\tAverage training loss: 0.05112\n",
      "\tVal Accuracy: 0.8920\n",
      "Iter: 901\n",
      "\tAverage training loss: 0.05861\n",
      "\tVal Accuracy: 0.8905\n",
      "Iter: 1001\n",
      "\tAverage training loss: 0.06632\n",
      "\tVal Accuracy: 0.8920\n",
      "Iter: 1101\n",
      "\tAverage training loss: 0.05918\n",
      "\tVal Accuracy: 0.8913\n",
      "Iter: 1201\n",
      "\tAverage training loss: 0.06067\n",
      "\tVal Accuracy: 0.8920\n",
      "Epoch: 3\n",
      "Reducing LR\n",
      "Iter: 1\n",
      "\tAverage training loss: 0.01537\n",
      "\tVal Accuracy: 0.8913\n",
      "Iter: 101\n",
      "\tAverage training loss: 0.03983\n",
      "\tVal Accuracy: 0.8904\n",
      "Iter: 201\n",
      "\tAverage training loss: 0.04255\n",
      "\tVal Accuracy: 0.8915\n",
      "Iter: 301\n",
      "\tAverage training loss: 0.05131\n",
      "\tVal Accuracy: 0.8904\n",
      "Iter: 401\n",
      "\tAverage training loss: 0.04360\n",
      "\tVal Accuracy: 0.8905\n",
      "Iter: 501\n",
      "\tAverage training loss: 0.03499\n",
      "\tVal Accuracy: 0.8911\n",
      "Iter: 601\n",
      "\tAverage training loss: 0.03977\n",
      "\tVal Accuracy: 0.8910\n",
      "Iter: 701\n",
      "\tAverage training loss: 0.04768\n",
      "\tVal Accuracy: 0.8914\n",
      "Iter: 801\n",
      "\tAverage training loss: 0.04261\n",
      "\tVal Accuracy: 0.8902\n",
      "Iter: 901\n",
      "\tAverage training loss: 0.04331\n",
      "\tVal Accuracy: 0.8908\n",
      "Iter: 1001\n",
      "\tAverage training loss: 0.04616\n",
      "\tVal Accuracy: 0.8912\n",
      "Iter: 1101\n",
      "\tAverage training loss: 0.04573\n",
      "\tVal Accuracy: 0.8919\n",
      "Iter: 1201\n",
      "\tAverage training loss: 0.05002\n",
      "\tVal Accuracy: 0.8893\n",
      "Epoch: 4\n",
      "Iter: 1\n",
      "\tAverage training loss: 0.06182\n",
      "\tVal Accuracy: 0.8908\n",
      "Iter: 101\n",
      "\tAverage training loss: 0.03939\n",
      "\tVal Accuracy: 0.8900\n",
      "Iter: 201\n",
      "\tAverage training loss: 0.03846\n",
      "\tVal Accuracy: 0.8913\n",
      "Iter: 301\n",
      "\tAverage training loss: 0.03545\n",
      "\tVal Accuracy: 0.8909\n",
      "Iter: 401\n",
      "\tAverage training loss: 0.03934\n",
      "\tVal Accuracy: 0.8902\n",
      "Iter: 501\n",
      "\tAverage training loss: 0.04030\n",
      "\tVal Accuracy: 0.8909\n",
      "Iter: 601\n",
      "\tAverage training loss: 0.03453\n",
      "\tVal Accuracy: 0.8923\n",
      "Iter: 701\n",
      "\tAverage training loss: 0.03863\n",
      "\tVal Accuracy: 0.8906\n",
      "Iter: 801\n",
      "\tAverage training loss: 0.04016\n",
      "\tVal Accuracy: 0.8918\n",
      "Iter: 901\n",
      "\tAverage training loss: 0.04143\n",
      "\tVal Accuracy: 0.8897\n",
      "Iter: 1001\n",
      "\tAverage training loss: 0.04176\n",
      "\tVal Accuracy: 0.8918\n",
      "Iter: 1101\n",
      "\tAverage training loss: 0.04347\n",
      "\tVal Accuracy: 0.8932\n",
      "Iter: 1201\n",
      "\tAverage training loss: 0.04095\n",
      "\tVal Accuracy: 0.8924\n",
      "Final Training Accuracy: 0.9923\n",
      "Final Validation Accuracy: 0.8911\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "for i in range(config.max_epochs):\n",
    "    print (\"Epoch: {}\".format(i))\n",
    "    train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "train_acc = evaluate_model(model, dataset.train_iterator)\n",
    "val_acc = evaluate_model(model, dataset.val_iterator)\n",
    "\n",
    "print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
    "print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "\n",
    "def report(Dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for idx,batch in enumerate(Dataset.val_iterator):\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "        else:\n",
    "            x = batch.text\n",
    "    y_pred = model(x)\n",
    "    predicted = torch.max(y_pred.cpu().data, 1)[1]\n",
    "    all_preds.extend(predicted.numpy())\n",
    "    all_y.extend(batch.label.numpy())\n",
    "    predicted_df = pd.DataFrame(all_preds)\n",
    "    all_y_df = pd.DataFrame(all_y)\n",
    "    all_preds = dataset.ordinal_encoder.inverse_transform(predicted_df)\n",
    "    all_y = dataset.ordinal_encoder.inverse_transform(all_y_df)\n",
    "    print(metrics.classification_report(all_y,all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "                     ANIMALERIE - NEW       1.00      1.00      1.00         1\n",
      "ART DE LA TABLE - ARTICLES CULINAIRES       1.00      1.00      1.00         1\n",
      "                    AUTO - MOTO (NEW)       0.00      0.00      0.00         0\n",
      "         BIJOUX -  LUNETTES - MONTRES       1.00      1.00      1.00         2\n",
      "BRICOLAGE - OUTILLAGE - QUINCAILLERIE       0.00      0.00      0.00         2\n",
      "             CHAUSSURES - ACCESSOIRES       0.00      0.00      0.00         0\n",
      "                       CULTURE / JEUX       1.00      1.00      1.00         1\n",
      "                       ELECTROMENAGER       0.00      0.00      0.00         0\n",
      "                         INFORMATIQUE       1.00      1.00      1.00         1\n",
      "                          JOUET (NEW)       0.00      0.00      0.00         1\n",
      "                            LIBRAIRIE       0.50      1.00      0.67         1\n",
      "                             MERCERIE       1.00      1.00      1.00         3\n",
      "                      PHOTO - OPTIQUE       1.00      1.00      1.00         6\n",
      "                         PUERICULTURE       1.00      1.00      1.00         1\n",
      "                          SPORT (NEW)       1.00      1.00      1.00         1\n",
      "                     TELEPHONIE - GPS       1.00      1.00      1.00         6\n",
      "                VETEMENTS - LINGERIE        1.00      0.80      0.89         5\n",
      "\n",
      "                             accuracy                           0.88        32\n",
      "                            macro avg       0.68      0.69      0.68        32\n",
      "                         weighted avg       0.89      0.88      0.88        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rachi\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "report(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long short-term memory (LSTM) \n",
    "\n",
    "\n",
    "\n",
    "L’intérêt des RNN réside dans leur capacité à exploiter l’information contextuelle pour passer d’une séquence d’entrée à une séquence de sortie qui soit le plus proche possible de la séquence cible. Malheureusement, pour les RNN standards l’apprentissage peut se révéler difficile et le contexte réellement exploité très local. Le problème vient du fait qu’un vecteur d’entrée ne peut avoir une influence sur les décisions futures qu’au travers des liens récurrents et\n",
    "que par conséquent cette influence décroit ou augmente exponentiellement au fur et à mesure qu’on avance dans la séquence. Ce phénomène est souvent appelé vanishing gradient parce qu’il impacte la rétro-propagation du gradient.\n",
    "\n",
    "Dans les années 1990, de nombreuses architectures neuronales et méthodes d’apprentissage ont été testées pour essayer de contrer ce phénomène. Mais l’approche qui s’est montrée la plus efficace et\n",
    "qui est maintenant devenue la norme pour traiter des séquences est le modèle LSTM proposé par Hochreiter et Schmidhuber en 1997 . En effet, ce modèle introduit des portes logiques multiplicatives qui permettent de conserver et d’accéder à l’information\n",
    "pertinente sur de longs intervalles permettant ainsi de diminuer l’impact du problème de gradient évanescent.\n",
    "\n",
    "\n",
    "Dans la plupart des problèmes du monde réel, des variantes de RNN telles que LSTM ou GRU sont utilisées, qui résolvent les limites du RNN ordinaire et ont également la possibilité de mieux gérer les données séquentielles.\n",
    "\n",
    "\n",
    "Les LSTM  fonctionnent extrêmement bien sur une grande variété de problèmes et sont largement utilisés.Les LSTM sont conçus pour éviter les problèmes de dépendance à long terme en ayant une conception grâce à laquelle il est naturel de se souvenir des informations pendant une longue période.\n",
    "\n",
    "La figure montre la cellule de base d'un modèle LSTM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/LSTM.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Créez un réseau basé sur LSTM pour résoudre le problème de classification de texte sur notres ensembles de données \n",
    "le code complet est sur mon référentiel git. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture du modèle\n",
    "L'architecture du LSTM bidirectionnel est la suivante:\n",
    "<img src=\"data/BILSTM.jpeg\" width=\"300\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Détails d'implémentation\n",
    "* fastText fr Embeddings pré-formés utilisés pour initialiser les vecteurs de mots\n",
    "* 2 couches de BiLSTM\n",
    "* Utilisé 32 unités cachées dans chaque couche BiLSTM\n",
    "* Dropou avec probabilité de conservation 0,8\n",
    "* Optimiseur - adam\n",
    "* Fonction de perte - entropie croisée (CrossEntropyLoss)\n",
    "* Expérimenté avec des longueurs de séquence flexibles et des séquences de longueur 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model_LSTM.utils import evaluate_model,Dataset\n",
    "from Model_LSTM.model import LSTMClassifier\n",
    "from Model_LSTM.config import Config\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "config = Config()\n",
    "train_file = \"data/cdiscount_train.csv.zip\"\n",
    "w2v_file = 'C:\\DEV\\Article\\data\\cc.fr.300.vec'\n",
    "\n",
    "dataset = Dataset(config)\n",
    "dataset.load_data(w2v_file, train_file)\n",
    "\n",
    "# Create Model with specified optimizer and loss function\n",
    "##############################################################\n",
    "model = LSTMClassifier(config, len(dataset.vocab), dataset.word_embeddings)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=config.lr)\n",
    "\n",
    "NLLLoss = nn.NLLLoss()\n",
    "\n",
    "\n",
    "model.add_optimizer(optimizer)\n",
    "model.add_loss_op(NLLLoss)\n",
    "\n",
    "    ##############################################################\n",
    "\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for i in range(config.max_epochs):\n",
    "        print(\"Epoch: {}\".format(i))\n",
    "        train_loss, val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    train_acc = evaluate_model(model, dataset.train_iterator)\n",
    "    val_acc = evaluate_model(model, dataset.val_iterator)\n",
    "\n",
    "\n",
    "    print('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
    "    print('Final Validation Accuracy: {:.4f}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mécanisme Attention\n",
    "\n",
    "L'attention est une autre astuce qui peut être mise en œuvre dans des modèles de séquence à séquence.\n",
    "Les modèles Seq2Seq sont particulièrement bons en traduction, où la séquence de mots d'une langue est transformée en une séquence de mots différents dans une autre langue. Un choix populaire pour ce type de modèle est les modèles basés sur la mémoire à long terme et à court terme (LSTM). Avec des données dépendantes de la séquence, les modules LSTM peuvent donner un sens à la séquence tout en se souvenant (ou en oubliant) les parties qu'elle juge importantes (ou sans importance). Les phrases, par exemple, dépendent de la séquence, car l'ordre des mots est crucial pour comprendre la phrase. Les LSTM sont un choix naturel pour ce type de données.\n",
    "\n",
    "Les modèles Seq2Seq se composent d'un encodeur et d'un décodeur. L'encodeur prend la séquence d'entrée et la mappe dans un espace de dimension supérieure (vecteur à n dimensions). Ce vecteur abstrait est introduit dans le décodeur qui le transforme en une séquence de sortie. La séquence de sortie peut être dans une autre langue, des symboles, une copie de l'entrée, etc.\n",
    "\n",
    "<img src=\"data/SEQ2SEQpng.png\" width=\"500\"/>\n",
    "\n",
    "Cependant, dans le modèle seq2seq, les informations source sont compressées dans un vecteur de contexte de longueur fixe. Un inconvénient critique de cette conception est l' incapacité de la mémoire des phrases longues et conduit à un mauvais résultat,Au fur et à mesure que le contexte passe par les pas de temps sur le décodeur le signal est combiné avec la sortie du décodeur et devient de plus en plus faible,Le résultat est que le contexte n'a pas beaucoup d'effet sur les pas de temps ultérieurs sur le décodeur. De plus, certaines sections de la sortie du décodeur peuvent dépendre plus fortement de certaines sections de l'entrée.\n",
    "\n",
    "\n",
    "Le modèle d'attention est né pour aider à mémoriser de longues phrases sources en traduction automatique neuronale.\n",
    "\n",
    "\n",
    "Le mécanisme d'attention examine une séquence d'entrée et décide à chaque étape quelles autres parties de la séquence sont importantes. Cela semble abstrait, mais permettez-moi de clarifier avec un exemple simple: lorsque vous lisez un texte, vous vous concentrez toujours sur le mot que vous lisez, mais en même temps, votre esprit conserve en mémoire les mots-clés importants du texte afin de fournir un contexte.\n",
    "\n",
    "Un mécanisme d'attention fonctionne de manière similaire pour une séquence donnée. \n",
    "En d'autres termes, pour chaque entrée lue par le LSTM (Encoder), le mécanisme d'attention prend en compte plusieurs autres entrées en même temps et décide lesquelles sont importantes (ou de prêter plus d'attention) en attribuant des poids différents à ces entrées. Le décodeur prendra alors en entrée la phrase codée et les poids fournis par le mécanisme d'attention.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"data/ATT.PNG\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre modèle de classification  l'encodeur sera les couches BILSTM et  le décodeur la couche linéaire dans notre suivit d'une softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture du modèle Seq2Seq avec Attention pour la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/BiLSTM-Attention.ppm\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Détails d'implémentation\n",
    "* fastText fr Embeddings pré-formés utilisés pour initialiser les vecteurs de mots\n",
    "* 2 couches de BiLSTM\n",
    "* Utilisé 64 unités cachées dans chaque couche BiLSTM\n",
    "* Dropou avec probabilité de conservation 0,4\n",
    "* Optimiseur - adam\n",
    "* Fonction de perte - entropie croisée (CrossEntropyLoss)\n",
    "* Séquences de longueur 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseau convolutionnel sur données séquentielles\n",
    "    \n",
    "   Les réseaux neuronaux convolutifs (CNN) ont été initialement conçus pour effectuer les tâches de vision par ordinateur, et se sont révélés très efficaces. Ils utilisent le concept de «convolution», une fenêtre coulissante ou un «filtre» qui passe sur l'image, identifiant les caractéristiques importantes et les analysant une à la fois, puis les réduisant à leurs caractéristiques essentielles et répétant le processus.\n",
    "    \n",
    "Ils se sont également révélés très utiles pour les tâches de traitement du langage naturel (NLP). En 2014, Yoon Kim a publié le document de recherche(https://www.aclweb.org/anthology/D14-1181/ )  sur l'utilisation des CNN pour la classification des textes. Il a testé quatre variations CNN et a montré que les modèles CNN pouvaient surpasser les approches précédentes pour plusieurs tâches de classification\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous trouverez ci-dessous une architecture CNN typique utilisée pour le traitement de texte. Cela commence par une phrase d'entrée décomposée en mots ou en embeddings de mots: des représentations de faible dimension générées par des modèles comme word2vec ou GloVe.\n",
    "\n",
    "Les mots sont décomposés en caractéristiques et sont introduits dans une couche convolutionnelle. Les résultats de la convolution sont «regroupés» ou agrégés à un nombre représentatif. Ce nombre est envoyé à une structure neuronale entièrement connectée, qui prend une décision de classification en fonction des poids attribués à chaque entité dans le texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/CNN.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un CNN, le texte est organisé en matrice, chaque ligne représentant un mot , un mot ou un caractère. La couche convolutionnelle du CNN «numérise» le texte comme s'il s'agissait d'une image, le décompose en entités et juge si chaque entité correspond ou non à l'étiquette appropriée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseaux convolutifs au niveau des caractères pour la classification de texte\n",
    "\n",
    "https://arxiv.org/abs/1509.01626\n",
    "    \n",
    "\n",
    "Une tendance est d'apprendre à utiliser des données brutes et de fournir aux modèles d'apprentissage automatique un accès à plus d'informations sur la structure du texte. Une prochaine étape logique serait d'introduire un flux de caractères dans le modèle et de le laisser tout apprendre sur les mots.\n",
    "\n",
    "Un avantage supplémentaire est que le modèle peut apprendre les fautes d'orthographe et les émoticônes. En outre, le même modèle peut être utilisé pour différentes langues, même celles où la segmentation en mots n'est pas possible\n",
    "\n",
    "\n",
    "L'article « Character-level Convolutional Networks for Text Classification (ConvNets)  https://arxiv.org/abs/1509.01626 » explore l'utilisation des réseaux ConvNet au niveau des caractères pour la classification des textes. Ils comparent les performances de quelques modèles différents sur plusieurs ensembles de données à grande échelle.\n",
    "\n",
    "Les résultats sont assez intéressants. Les modèles TFIDF et N-gramme sont les meilleurs pour les petits ensembles de données, jusqu'à plusieurs centaines de milliers d'échantillons. Mais lorsque la taille de l'ensemble de données atteint plusieurs millions, nous pouvons observer que ConvNet au niveau des caractères fonctionne mieux.\n",
    "\n",
    "\n",
    "ConvNet a tendance à mieux fonctionner pour les textes qui sont moins organisés. Le choix de l'alphabet est important. Le meilleur fonctionnement de ConvNet est de ne pas distinguer les majuscules des minuscules.\n",
    "\n",
    "<img src=\"data/convnet.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vous trouvez sur mon repot git une mise en œuvre de ConvNet comme proposé dans l'article Character-level Convolutional Networks for Text Classification .\n",
    "\n",
    "Dans CharCNN, le texte d'entrée est représenté par une matrice ( l_0 , d ). Où l_0 est la longueur de phrase maximale et d est la dimensionnalité de l'incorporation de caractères.\n",
    "\n",
    "Les caractères suivants sont utilisés pour la quantification des caractères:\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}\n",
    "\n",
    "\n",
    "L'architecture de CharCNN comprend 9 couches: 6 couches convolutionnelles et 3 couches entièrement connectées. L'entrée a 70 caractéristiques, en raison de la méthode de quantification des caractères ci-dessus et la longueur de la caractéristique d'entrée l_0 est choisie pour être 300 (1014 dans le papier original). 2 couches de suppression sont insérées entre les 3 couches entièrement connectées.\n",
    "\n",
    "L'architecture de CharCNN comporte 9 couches : 6 couches convolutives et 3 couches entièrement connectées. L'entrée a 70 caractéristiques,la longueur de la caractéristique d'entrée l_0 est choisie pour être de 300 (1014 dans le document original). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "“Attention is All you Need” (Vaswani, et al., 2017), sans aucun doute, est l'un des articles les plus percutants et intéressants en 2017. Il a présenté de nombreuses améliorations à l'attention et permet de faire une  modélisation seq2seq sans unités de réseau récurrentes. Le modèle de « Transformer » proposé est entièrement construit sur les mécanismes d'auto-attention sans utiliser d'architecture récurrente alignée sur la séquence.\n",
    "\n",
    "Comme son titre l'indique, il utilise le mécanisme d'attention que nous avons vu précédemment. Transformer est une architecture pour transformer une séquence en une autre à l'aide de deux parties (encodeur et décodeur), mais elle diffère des modèles de séquence à séquence précédemment décrits / existants car elle n'implique aucun réseau récurrent ( GRU, LSTM, etc.).\n",
    "Les réseaux récurrents étaient, jusqu'à présent, l'un des meilleurs moyens de capturer les dépendances en temps voulu dans des séquences dans les séquences. Cependant, l'équipe qui a présenté l'article a prouvé qu'une architecture avec uniquement des mécanismes d'attention sans RNN (réseaux de neurones récurrents) peut améliorer les résultats de la tâche de traduction et d'autres tâches! \n",
    "\n",
    "<img src=\"data/trans.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "L'encodeur est à gauche et le décodeur à droite. L'encodeur et le décodeur sont composés de modules qui peuvent être empilés les uns sur les autres plusieurs fois, ce qui est décrit par Nx dans la figure. Nous voyons que les modules se composent principalement de couches Multi-Head Attention et Feed Forward. Les entrées et sorties (phrases cibles) sont d'abord intégrées dans un espace à n dimensions car nous ne pouvons pas utiliser directement des chaînes.\n",
    "Une partie légère mais importante du modèle est l'encodage positionnel des différents mots. Comme nous n'avons pas de réseaux récurrents qui peuvent se rappeler comment les séquences sont introduites dans un modèle, nous devons en quelque sorte donner à chaque mot / partie de notre séquence une position relative, car une séquence dépend de l'ordre de ses éléments. Ces positions sont ajoutées à la représentation intégrée (vecteur à n dimensions) de chaque mot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une amélioration des tâches en langage naturel est présentée par une équipe présentant BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://arxiv.org/abs/1810.04805\n",
    "\n",
    "\n",
    "Dernièrement, plusieurs méthodes ont été présentées pour améliorer le BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les algorithmes basés sur BERT les plus connus :\n",
    "\n",
    "* **RoBERTa** par Facebook, créé par Liu et al\n",
    "* **MT-DNN** par Microsoft (détails)\n",
    "* **XLNet and ALBERT** par Google et Toyota. Sorti en septembre 2019, ALBERT est déjà considéré comme le successeur de BERT, qu’il surpasse dans tous les domaines (notamment en termes de score sur SQuAD 2.0)\n",
    "* **BERT-mtl** par IBM\n",
    "* **Google T5** par Google\n",
    "* **DistilBERT** est une version plus petite, légère et rapide de BERT\n",
    "* **FastBERT** est une version plus rapide de BERT\n",
    "* **CamemBERT** est une version française développée par l’INRIA, dérivée de RoBERTa (détails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan=\"3\">Model</th>\n",
    "    <th align=\"center\" colspan=\"4\">Dataset</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\"> </th>\n",
    "    <th colspan=\"2\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Accuracy</th>\n",
    "    <th>precision</th>\n",
    "    <th>recall</th>\n",
    "    <th>f1-score </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>fastText</td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>CNNClassifier</td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>RNNClassifier</td>\n",
    "    <td>  </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>CNNClassifier</td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>CharCNN</td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LSTMAttClassifier</td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Transformer</td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "    <td> </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
