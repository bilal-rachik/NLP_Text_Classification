{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing)-machine learning  1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce travaille il s’agit d’un guide pour le traitement de texte en Francais,\n",
    "\n",
    "Cet article vous présentera le vocabulaire de base et un flux de travail suggéré pour la création d'applications NLP afin de vous aider à démarrer avec les tâches les plus courantes telles, l'analyse des sentiments, la reconnaissance des entités,wordembeddings et la traduction automatique ...\n",
    "\n",
    "Vous découvrirez également deux méthodes populaires d'extraction de fonctionnalités: le bag of words et  TF-IDF, ainsi que diverses méthodes permettant de créer de nouvelles fonctionnalités à partir de fonctionnalités existantes. Enfin, vous vous familiariserez avec la façon dont les données de texte peuvent être visualisées, Dans l’esprit et de pouvoir construire les choses.\n",
    "D’abord, nous allons apprendre à construire un système de classification de texte simple à l’aide de scikit-learn de Python. Cet article est un bon choix pour vous. Allons-y!.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’analyse de texte moderne est désormais très accessible à l’aide de Python et d’outils open source, qui vous permettent de découvrir comment analyser le texte moderne à l’ère des données textuelles.Cet article vous explique comment utiliser le traitement du langage naturel et des algorithmes de calcul informatique pour faire des inférences et mieux comprendre vos données. Les algorithmes sont basés sur des techniques d’apprentissage statistique et d’intelligence artificielle.\n",
    "\n",
    "Les outils permettant de travailler avec ces algorithmes sont maintenant disponibles avec Python, tels que Gensim et spaCy,NLTK .Vous allez commencer par apprendre le nettoyage des données, puis comment exécuter ces tâches. calculinguistique à partir des premiers concepts.\n",
    "\n",
    "Vous êtes alors prêt à explorer les domaines les plus sophistiqués de NLP statistique et de machine learning  en utilisant Python, en utilisant un langage et des exemples de texte réalistes. Vous apprendrez à baliser, analyser et modéliser du texte à l'aide des meilleurs outils. Vous obtiendrez une connaissance pratique des meilleurs frameworks à utiliser, et vous saurez quand choisir un outil tel que Gensim pour les modèles de sujets et quand travailler avec Keras,pytorch pour deep learnig. Cet article présente un équilibre entre théorie et exemples pratiques. Vous pourrez ainsi mieux connaître et mener vos propres projets de traitement du langage naturel et la linguistique informatique.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installation \n",
    "Avant de commencer cet article , nous allons scikit-learn et les autres bibliothèques utilisées dans cet article. Vous trouverez les étapes pour les installer ici:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diverses étapes de NLP\n",
    "\n",
    "La plupart du temps, les données textuelles ne peuvent pas être utilisées telles quelles. Cela est dû au fait que la présence de divers symboles ou liens inconnus le rend impropre à l’utilisation. Le nettoyage des données est l'art d'extraire des parties significatives des données en éliminant les détails inutiles. Différents symboles, tels que \"_ / \\ _\" et \":),\".Ces symboles par exemple,  Ils ne contribuent pas beaucoup à sa signification. Nous devons supprimer ces détails indésirables. Ceci est fait non seulement pour se concentrer davantage sur le contenu réel mais également pour réduire les calculs. Pour y parvenir, des méthodes telles que la tokénisation et stemming sont utilisées.\n",
    ".Nous discuterons en détail de diverses tâches de prétraitement et les démontrerons avec des exemples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization\n",
    "\n",
    "Étant donné une phrase, la diviser en caractères ou en mots s'appelle la tokénisation. Il existe des bibliothèques, telles que spaCy, qui offrent des solutions complexes à la Tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp = spacy.load('fr')\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  la tokinzation d’un texte en phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ceci est 1 première phrase.', \"Puis j'en écris une seconde.\", 'pour finir en voilà une troisième sans mettre de majuscule']\n"
     ]
    }
   ],
   "source": [
    "# On crée un texte composé de plusieurs phrases\n",
    "text_fr = \"Ceci est 1 première phrase. Puis j'en écris une seconde. pour finir en voilà une troisième sans mettre de majuscule\"\n",
    "# On passe le texte par le pipeline\n",
    "doc = nlp(text_fr)\n",
    "# On affiche les phrases\n",
    "sents =[sent.text for sent in doc.sents]\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### la tokinzation d’un texte en tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ceci', 'est', '1', 'première', 'phrase', '.', 'Puis', \"j'\", 'en', 'écris', 'une', 'seconde', '.', 'pour', 'finir', 'en', 'voilà', 'une', 'troisième', 'sans', 'mettre', 'de', 'majuscule']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.tokenizer(text_fr)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversion de texte en caractères\n",
    "\n",
    "La fonction list Python prend une chaîne et la convertit en une liste de caractères individuels. Ceci permet de convertir le texte en caractères. Le bloc de code suivant montre le code utilisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'e', 'c', 'i', ' ', 'e', 's', 't', ' ', '1', ' ', 'p', 'r', 'e', 'm', 'i', 'è', 'r', 'e', ' ', 'p', 'h', 'r', 'a', 's', 'e']\n"
     ]
    }
   ],
   "source": [
    "text_fr = \"Ceci est 1 première phrase\"\n",
    "# toz(text)\n",
    "doc = list(text_fr)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Traitement de gros volumes de texte\n",
    "Si vous devez traiter un grand nombre de textes et créer plusieurs objets Doc à la suite, la méthode nlp.pipe peut accélérer considérablement cette opération.\n",
    "Il traite les textes en tant que flux et génère des objets Doc.\n",
    "C’est beaucoup plus rapide que d’appeler nlp sur chaque texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ceci', 'est', '1', 'première', 'phrase', '.'], ['Puis', \"j'\", 'en', 'écris', 'une', 'seconde', '.'], ['pour', 'finir', 'en', 'voilà', 'une', 'troisième', 'sans', 'mettre', 'de', 'majuscule']]\n"
     ]
    }
   ],
   "source": [
    "data = ['Ceci est 1 première phrase.',\n",
    "        \"Puis j'en écris une seconde.\",\n",
    "        'pour finir en voilà une troisième sans mettre de majuscule']\n",
    "\n",
    "doc = nlp.pipe(data)\n",
    "tokens = [[token.text for token in tokens] for tokens in doc]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Enlever les mots les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mots vides  (stop words) sont des mots courants qui sont simplement utilisés pour faciliter la construction de phrases. Comme ils se produisent très fréquemment et que leur présence n'a pas beaucoup d'impact sur le sens de la phrase, ils doivent être supprimés. \n",
    "Les « stop words » sont établis comme des listes de mots. Ces listes sont généralement disponibles dans une librairie appelée spacy, et dans beaucoup de langues différentes.\n",
    "\n",
    "Il n'y a pas de liste de mots vides universels pour chaque langue, et cela dépend en grande partie du cas d'utilisation et du type de résultats escomptés. En général, il s'agit d'une liste des mots les plus communs dans la langue. Avec spaCy, les mots vides sont très faciles à identifier - chaque jeton possède un attribut IS_STOP, qui nous permet de savoir si le mot est un mot vide ou non. Nous pouvons également ajouter nos propres mots vides à la liste des mots vides.\n",
    "\n",
    "On accède aux listes en français de cette manière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 600\n",
      "First ten stop words: ['compris', 'chaque', 'dix-sept', 'parseme', 'quel', 'auxquels', 'pfft', 'chacun', 'dessous', 'exactement']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "print('Number of stop words: %d' % len(STOP_WORDS))\n",
    "print('First ten stop words: %s' % list(STOP_WORDS)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Supprimer les mots vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: ['Ceci est 1 première phrase.', \"Puis j'en écris une seconde.\", 'pour finir en voilà une troisième sans mettre de majuscule']\n",
      "[['1', 'phrase', '.'], ['écris', 'seconde', '.'], ['finir', 'mettre', 'majuscule']]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.pipe(data)\n",
    "tokens = [[token.text for token in tokens  if not token.is_stop  ] for tokens in doc]\n",
    "print('Original Article: %s' % (data))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajouter des mots vides personnalisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: ['Ceci est 1 première phrase.', \"Puis j'en écris une seconde.\", 'pour finir en voilà une troisième sans mettre de majuscule']\n",
      "[['phrase'], ['écris', 'seconde'], ['finir', 'mettre', 'majuscule']]\n"
     ]
    }
   ],
   "source": [
    "customize_stop_words = ['1', '.']\n",
    "\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True\n",
    "doc = nlp.pipe(data)\n",
    "tokens = [[token.text for token in tokens  if not token.is_stop  ] for tokens in doc]\n",
    "print('Original Article: %s' % (data))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Normalization\n",
    "\n",
    "Certains mots sont épelés, prononcés et représentés différemment, par exemple Mumbai et Bombay, ainsi que US et États-Unis. Bien qu'ils soient différents, ils veulent dire la même chose. Il existe également différentes formes de mots qui doivent être convertis en formes de base. Par exemple, des mots tels que \"fait\" et \"fesait\", une fois convertis en leur forme de base, deviennent \"faire\". Dans cet ordre d'idées, la normalisation du texte est un processus dans lequel différentes variations de texte sont converties en un formulaire standard. Nous devons effectuer la normalisation du texte car certains mots peuvent signifier la même chose. Il existe différentes manières de normaliser le texte, telles que la correction orthographique, stemming et lemmatization. Pour une meilleure compréhension de ce sujet, nous examinerons la mise en œuvre pratique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### souvent tout en minuscule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ceci', 'est', '1', 'première', 'phrase', '.'], ['puis', \"j'\", 'en', 'écris', 'une', 'seconde', '.'], ['pour', 'finir', 'en', 'voilà', 'une', 'troisième', 'sans', 'mettre', 'de', 'majuscule']]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.pipe(data)\n",
    "tokens = [[token.text.lower() for token in tokens] for tokens in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"J'ai visité les US le 22-10-18\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulons remplacer \"US\" par \"États-Unis\",et \"18\" par \"2018\". Pour ce faire, nous utilisons la fonction replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"j'\", 'ai', 'visité', 'les', 'états-unis', 'le', '22', '-', '10', '-', '2018']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sentence)\n",
    "\n",
    "tokens = [token.text.replace(\"US\", \"États-Unis\").\n",
    "          replace(\"18\", \"2018\").lower()\n",
    "          for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous devez faire correspondre de grandes listes de terminologie, vous pouvez également utiliser l'objets **PhraseMatcher** de spacy pour transforme les expressions multi-mots courantes en éléments simples\n",
    "\n",
    "**\"New\",\"Yourk\"** ==> **\"new york\"**\n",
    "ce qui est beaucoup plus efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'fr'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-420e73d5ac54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'fr'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('fr')\n",
    "\n",
    "\n",
    "class Phraser(object):\n",
    "    def __init__(self, nlp,terms):\n",
    "        patterns =[nlp.make_doc(text) for text in terms]\n",
    "        self.matcher =  PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "        self.matcher.add(\"TerminologyList\", None, *patterns)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        # this will be called on the Doc object in the pipeline\n",
    "        matched_spans = []\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            matched_spans.append(span)\n",
    "        for span in matched_spans:  # merge into one token after collecting all matches\n",
    "            span.merge()\n",
    "        return doc\n",
    "    \n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]    \n",
    "Phrase = Phraser(nlp,terms)\n",
    "nlp.add_pipe(Phrase,first=True)\n",
    "\n",
    "doc = nlp(\" Angela Merkel et le président américain Barack Obama\" \n",
    "          \" sont à l'intérieur de la Maison Blanche à Washington, D.C.\")\n",
    "\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque spaCy est utilisé pour traiter à la fois les motifs et le texte à faire correspondre, vous n'aurez pas à vous soucier de la tokenisation spécifique - par exemple ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming et Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors du nettoyage de notre texte, nous pouvons simplement choisir de ne pas ajouter des mots vides à notre corpus.\n",
    "\n",
    "Par exemple, si Vous remarquez les mots \"disent\", \"disant\", et \"dit\" tous fournissent à peu près la même information pour nous - à part les différences grammaticales.\n",
    "nos résultats ne conduisent pas à une seule représentation de ces mots.\n",
    "\n",
    "Il existe deux techniques populaires pour y parvenir, le stemming et la lemmatisation.\n",
    "\n",
    "Stemming implique généralement de couper la fin du mot, en respectant quelques règles de base. Par exemple, les mots disent, disant, et disaitt deviendraient tous dis. Stemming est sans contexte et ne compte pas sur une partie du discours , pour prendre ses décisions.\n",
    "La lemmatisation, en revanche, effectue une analyse morphologique pour trouver le mot racine.\n",
    ". En ce qui nous concerne, nous n’avons pas besoin de nous inquiéter d’où nous venons d’obtenir nos mots-clés, mais simplement de les obtenir dans SpaCy, la forme lemmatisée d'un mot est accessible avec l'attribut .lemma_.\n",
    "\n",
    "Maintenant, avec ce que nous savons, nous pouvons effectuer un prétraitement de base.\n",
    "\n",
    "Nettoyons notre data: nous voudrions nous débarrasser des mots vides, des chiffres et convertir notre chaîne en liste afin de pouvoir l'utiliser plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['phrase'], ['écrire', 'second'], ['finir', 'mettre', 'majuscule']]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.pipe(data)\n",
    "tokens = [[w.lemma_ for w in tokens if w.text != 'n' and not w.is_stop and not w.is_punct and not w.like_num] for tokens in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les attributs .is_stop, is_punct et w.like_num, nous pourrions supprimer les parties de la phrase dont nous n'avions pas besoin.\n",
    "Assurez-vous de noter que nous avons ajouté à la phrase, la forme matimisée du mot auquel nous avons accédé via w.lemma_.\n",
    "Nous pouvons en outre supprimer ou non les mots en fonction de notre cas d'utilisation.\n",
    "Dans notre exemple, on estime que les chiffres ne sont pas des informations importantes, mais dans certains cas, ce peut être le cas.\n",
    "Par exemple, il se peut que nous voulions supprimer tous les verbes d’une phrase,dans ce cas, nous pouvons simplement vérifier l’étiquette POS de ce jeton particulier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu comment le texte peut être représenté sous forme de caractères et de mots. Parfois, il est utile de regarder deux, trois ou plusieurs mots ensemble. Les N-grammes sont des groupes de mots extraits d'un texte donné.\n",
    "\n",
    "Par exemple, considérez la phrase, \"Je ne te déteste pas, mais ton comportement.\"\n",
    "\n",
    "Ici, si nous traitons chacun des jetons, tels que «déteste» et «comportement», séparément, alors le vrai sens de la phrase ne serait pas compris. Dans ce cas, le contexte dans lequel ces jetons sont présents devient essentiel.\n",
    "\n",
    "Ainsi, nous considérons n jetons consécutifs à la fois. Dans un n-gramme, n représente le nombre de mots qui peuvent être utilisés ensemble. Regardons un exemple de ce à quoi ressemble un bigramme (n = 2). Nous avons utilisé le package Python nltk pour générer un bigramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Le', 'chien'), ('chien', 'était'), ('était', 'assis'), ('assis', 'près'), ('près', 'du'), ('du', 'tapis')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "phrase =\"Le chien était assis près du tapis\"\n",
    "print(list(ngrams(phrase.split(),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Extraction des Features à partir de textes\n",
    "\n",
    "Comprenons l'extraction de Features avec des exemples réels.\n",
    "Les Features représentent les caractéristiques d'une personne ou d'une chose. Ces caractéristiques peuvent représenter  de façon unique ou non, une personne ou une chose. Par exemple, les caractéristiques générales d'une personne, comme le nombre d'oreilles, des mains et des jambes, ne sont généralement pas suffisantes pour identifier de manière unique une personne . Mais des caractéristiques telles que les empreintes digitales et les séquences d'ADN peuvent être utilisées pour reconnaître distinctement cette personne. De même, dans l'extraction des Features, nous essayons d'extraire des attributs de textes qui représentent ces textes de manière unique. Les algorithmes d'apprentissage automatique ne prennent en entrée que des caractéristiques numériques, il faut donc eprésenter le texte en représentation numérique.\n",
    " \n",
    "Le processus de conversion de la représentation intonumérique du texte est appelé vectorisation et peut être effectué de différentes manières, comme indiqué ici:\n",
    "\n",
    "* Convertir du texte en mots et représenter chaque mot comme un vecteur\n",
    "\n",
    "* Convertissez le texte en caractères et représentez chaque caractère comme un vecteur\n",
    "\n",
    "* Créer n-gramme de mots et les représenter comme vecteurs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bag-of-word\n",
    "\n",
    "Le modèle du Bag of word est sans doute la forme la plus simple de représentation d’une phrase en tant que vecteur.\n",
    "Commençons par un exemple:\n",
    "\n",
    "* S1: \"Le chien était assis près du tapis\".\n",
    "* S2: \"Le chat aime le chien. \"\n",
    "\n",
    "Si nous suivons les mêmes étapes de prétraitement que nous avons suivies dans la section Prétraitement , nous obtiendrons les phrases suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chien', 'asseoir', 'tapis']\n",
      "['chat', 'aime', 'chien']\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Le chien était assis près du tapis\")\n",
    "doc2 = nlp(\"Le chat aime le chien.\")\n",
    "\n",
    "\n",
    "tokens1 = [w.lemma_.lower() for w in doc1 if w.text != 'n' and not w.is_stop and not w.is_punct and not w.like_num] \n",
    "tokens2 = [w.lemma_.lower() for w in doc2 if w.text != 'n' and not w.is_stop and not w.is_punct and not w.like_num] \n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous voulons représenter cela en tant que vecteur, nous devons d'abord construire notre vocabulaire, ce qui serait les mots uniques que l'on trouve dans les phrases. Notre vecteur de vocabulaire est maintenant comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chat', 'aime', 'asseoir', 'tapis', 'chien']\n"
     ]
    }
   ],
   "source": [
    "print(list(set(tokens1+tokens2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela signifie que notre représentation de nos phrases sera également des vecteurs d'une longueur de 5 - nous pouvons également dire que nos vecteurs auront 5 dimensions. Nous pouvons également penser à faire correspondre chaque mot de notre vocabulaire à un nombre (ou index), auquel cas nous pouvons également nous référer à notre vocabulaire comme à un dictionnaire.\n",
    "\n",
    "Le modèle du sac de mots implique l'utilisation de fréquences de mots pour construire nos vecteurs. À quoi ressembleront nos phrases maintenant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S1: [1, 0, 1, 1, 0]  \n",
    "S2: [0, 1, 1, 0, 1]\n",
    "\n",
    "C'est assez simple à comprendre, il y a 1 occurrence de chien dans les deux phrases, et 0 occurrence de mot \"aime\" dans la première phrase, de sorte que les index appropriés reçoivent la valeur basée sur la fréquence du mot.\n",
    "Si la première phrase a 2 occurrences du mot chien, elle serait représentée comme suit .\n",
    "\n",
    "S1:[1, 0, 2, 1, 0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une caractéristique importante du modèle de bag of word  dont nous devons nous rappeler, est qu'il s'agit d'une représentation moins ordonnée,il ne prend pas en compte l'ordre des mots.\n",
    "Nous ne savons pas quels mots sont venus en premier. Cela entraîne une perte d'informations spatiales et, par extension, d'informations sémantiques. Cependant, dans de nombreux algorithmesd, l’ordre des mots n’est pas important et seules les occurrences des mots suffisent pour commencer.\n",
    "\n",
    "Le filtre de courrier indésirable est un exemple d'utilisation du modèle de bag-of-word : les courriers électroniques considérés comme des courriers indésirableq sont susceptibles de contenir des mots en rapport avec les courriers indésirables, tels que achat, argent et actions.nous pouvons utiliser la probabilité bayésienne pour déterminer s'il est plus probable qu'un courrier se trouve ou non dans le dossier de courrier indésirable. Cela fonctionne car, comme nous l'avons vu précédemment, dans ce cas, l'ordre des mots n'a pas d'importance ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Auparavant, nous avons examiné le modèle bag-of-words. Ce modèle présente un grave inconvénient. La fréquence d'apparition d'un jeton ne représente pas entièrement la quantité d'informations qu'il contient sur un document. En effet, un terme apparaissant plusieurs fois dans de nombreux phrases ne transmet pas beaucoup d'informations. Les termes rares peuvent contenir beaucoup plus d'informations sur les phrases dans lesquels ils se trouvent.\n",
    "\n",
    "Le TF-IDF permet de faire ressortir l'importance relative de chaque mot $m$ (ou couples de mots consécutifs) dans un texte $d$. La fonction $TF(m,d)$ compte le nombre d'occurences du mot $m$ dans le descriptif $d$. La fonction $IDF(m)$ mesure l'importance du terme dans l'ensemble de document en donnant plus de poids aux termes les moins fréquents car considérés comme les plus discriminants (motivation analogue à celle de la métrique du chi2 en anamlyse des correspondance). $IDF(m,l)=\\log\\frac{D}{f(m)}$ où $D$ est le nombre de documents, la taille de l'échantillon d'apprentissage, et $f(m)$ le nombre de documents ou descriptifs contenant le mot $m$. La nouvelle variable ou features est $V_m(l)=TF(m,l)\\times IDF(m,l)$.\n",
    "\n",
    "\n",
    "\n",
    "Il existe deux approches populaires pour mapper des jetons générés à des vecteurs numériques, appelés **one-hot encoding** et **word embedding**. Je vais vous parler de ces méthodes dans le prochain article. Nous discuterons également des différents avantages et inconvénients de chaque méthode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering est une méthode d'extraction de nouvelles fonctionnalités à partir de fonctionnalités existantes. Ces nouvelles fonctionnalités sont extraites car elles tendent à expliquer efficacement la variabilité des données. Une application de Feature Engineering pourrait être de calculer la similarité des différents morceaux de texte. Il existe différentes manières de calculer la similarité entre deux textes. Les méthodes les plus populaires sont la similarité cosinus et la similitude Jaccard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitude cosinus: \n",
    "    La similitude cosinus entre deux textes est le cosinus de l'angle entre leurs représentations vectorielles. Les matrices BoW et TF-IDF peuvent être considérées comme des représentations vectorielles de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La Similité de  Jaccard:\n",
    "il s'agit du rapport entre le nombre de termes communs entre deux documents texte et le nombre total de termes uniques présents dans ces textes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet article, nous avons appris différentes étapes pour  gérer des données textuelles non structurées. Les données textuelles sont généralement en désordre et doivent être nettoyées et prétraitées. Les étapes de prétraitement consistent principalement en la tokenisation, le stemming, la lemmatisation et la suppression des mots vides. Après le prétraitement, les fonctionnalités sont extraites de textes à l'aide de diverses méthodes, telles que BoW et TF-IDF. Cette étape convertit les données textuelles non structurées en données numériques structurées. De nouvelles fonctionnalités sont créées à partir de fonctionnalités existantes à l'aide d'une technique appelée Feature Engineering.\n",
    "\n",
    "Dans l'article suivant vous apprendra quelques bonnes pratiques d'apprentissage automatique clés pour résoudre les problèmes de classification de texte. Voici ce que vous apprendrez:\n",
    "\n",
    "* Le flux de travail de haut niveau de bout en bout pour résoudre les problèmes de classification de texte à l'aide de l'apprentissage automatique\n",
    "* Comment choisir le bon modèle pour votre problème de classification de texte\n",
    "* Comment implémenter votre modèle de choix \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liens utiles :\n",
    "\n",
    "SpaCy\n",
    "NLTK\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
